{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# To make our relative library imports work\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustogerman/miniforge3/envs/narrative-trails/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from urllib.parse import unquote\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local Imports\n",
    "from Library.embedding_extraction import extract_embeddings\n",
    "from Library.narrative_landscape import NarrativeLandscape\n",
    "from Library.storyline import Storyline\n",
    "from metrics import similarity_metric, dtw_metric, absolute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # The filename for the source data\n",
    "    foldername = \"wikispeedia\"\n",
    "    # The column with the contents of the article\n",
    "    data_column = \"plain_text\"\n",
    "    # The \"date\" column. This column will be renamed to \"date\" later.\n",
    "    date_column = None\n",
    "    # The column used as title for visualization and summary\n",
    "    summary_column = \"title\"\n",
    "\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3928 entries, 0 to 3927\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   title       3928 non-null   object\n",
      " 1   plain_text  3928 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 61.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(f\"../../data/{CONFIG.foldername}/text_data.csv\")\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../../data/wikispeedia/embed_data-gpt4.pickle' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "RAW_TEXT = data[CONFIG.data_column]\n",
    "\n",
    "embeddings, _ = extract_embeddings(\n",
    "    text=RAW_TEXT.tolist(),\n",
    "    foldername=f\"../../data/{CONFIG.foldername}\",\n",
    "    model_name=\"gpt4\"  # OneOf[mpnet, gpt4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Narrative Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv(\n",
    "    \"../../data/Wikispeedia/original/links.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"src\", \"tgt\"]\n",
    ")\n",
    "\n",
    "links[\"src\"] = links[\"src\"].map(lambda x: unquote(x))\n",
    "links[\"tgt\"] = links[\"tgt\"].map(lambda x: unquote(x))\n",
    "\n",
    "links = links[(links[\"src\"].isin(data[\"title\"])) & (links[\"tgt\"].isin(data[\"title\"]))]\n",
    "\n",
    "ordered_data_titles = data[\"title\"].tolist()\n",
    "\n",
    "src_indices = links[\"src\"].map(lambda x: ordered_data_titles.index(x))\n",
    "tgt_indices = links[\"tgt\"].map(lambda x: ordered_data_titles.index(x))\n",
    "\n",
    "links_graph = nx.from_edgelist(list(zip(src_indices.tolist(), tgt_indices.tolist())))\n",
    "links_constraints = nx.adjacency_matrix(links_graph, nodelist=range(len(ordered_data_titles))).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Narrative Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/4: Constructing Projection Space with UMAP\n",
      "Step 2/4: Discovering topics with HDBSCAN\n",
      "\t>>> Discovered 87 Topics\n",
      "Step 3/4: Computing Mean K-NN Dist\n",
      "Step 4/4: Constructing Coherence Graph\n",
      "\t >>> Computing base coherence\n",
      "\t >>> Computing sparse coherence\n",
      "\t\t>>> Creating Undirected Graph\n",
      "\t\t>>> Finding Maximum Spanning Tree\n",
      "\t\t>>> Getting Min Weight\n",
      "\t\t----- BEFORE MST -----\n",
      "\t\tCritical Coherence: 0.5197929231231193\n",
      "\t\tNum Edges: 7712628\n",
      "\t\tIs Connected: True\n",
      "\t\t----- AFTER MST -----\n",
      "\t\tNum Edges: 3479213\n",
      "\t\tIs Connected: True\n",
      "\t\t----- AFTER Constraints -----\n",
      "\t\tNum Edges: 64117\n",
      "\t\tIs Connected: False\n",
      "\t >>> Building NetworkX graph\n"
     ]
    }
   ],
   "source": [
    "landscape = NarrativeLandscape(verbose=True)\n",
    "landscape.fit(embeddings.numpy(), constraints=links_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrative Landscape With Closeness Centrality Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/4: Constructing Projection Space with UMAP\n",
      "Step 2/4: Discovering topics with HDBSCAN\n",
      "\t>>> Discovered 87 Topics\n",
      "Step 3/4: Computing Mean K-NN Dist\n",
      "Step 4/4: Constructing Coherence Graph\n",
      "\t >>> Computing base coherence\n",
      "\t >>> Computing sparse coherence\n",
      "\t\t>>> Creating Undirected Graph\n",
      "\t\t>>> Finding Maximum Spanning Tree\n",
      "\t\t>>> Getting Min Weight\n",
      "\t\t----- BEFORE MST -----\n",
      "\t\tCritical Coherence: 0.20110551007990723\n",
      "\t\tNum Edges: 7712628\n",
      "\t\tIs Connected: True\n",
      "\t\t----- AFTER MST -----\n",
      "\t\tNum Edges: 4259941\n",
      "\t\tIs Connected: True\n",
      "\t\t----- AFTER Constraints -----\n",
      "\t\tNum Edges: 68250\n",
      "\t\tIs Connected: False\n",
      "\t >>> Building NetworkX graph\n"
     ]
    }
   ],
   "source": [
    "node_closeness = nx.closeness_centrality(links_graph)\n",
    "node_closeness = np.array(list(node_closeness.values()))\n",
    "\n",
    "landscape_closeness = NarrativeLandscape(verbose=True)\n",
    "landscape_closeness.fit(embeddings.numpy(), constraints=links_constraints, node_ranks=node_closeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrative Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = pd.read_csv(f\"../../data/{CONFIG.foldername}/ground_truth_paths.csv\")\n",
    "\n",
    "# Remove paths with duplicated documents (within the same path)\n",
    "ground_truths = ground_truths[ground_truths[\"path\"].map(\n",
    "    lambda x: not pd.Series(x.split(\";\")).duplicated().any()\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take hours to finish, so we save the extracted narratives to a pickle file\n",
    "try:\n",
    "    with open('./extracted_narratives.pkl', 'rb') as handle:\n",
    "        all_narratives = pickle.load(handle)\n",
    "except FileNotFoundError:\n",
    "    all_narratives = dict()\n",
    "    for idx, true_storyline in tqdm(enumerate(ground_truths[\"path\"]), total=len(ground_truths)):\n",
    "        true_storyline = true_storyline.split(\";\")\n",
    "        s = ordered_data_titles.index(unquote(true_storyline[0]))\n",
    "        t = ordered_data_titles.index(unquote(true_storyline[-1]))\n",
    "\n",
    "        if nx.has_path(landscape.nx_graph, s, t) and nx.has_path(landscape_closeness.nx_graph, s, t):\n",
    "            # Shortest path\n",
    "            sp = []\n",
    "            shortest_simple_paths = nx.shortest_simple_paths(landscape.nx_graph, s, t)\n",
    "            for _ in range(3):\n",
    "                sp_path = next(shortest_simple_paths, None)\n",
    "                if sp_path is not None:\n",
    "                    sp.append(sp_path)\n",
    "\n",
    "            # Regular narrative landscape\n",
    "            nt = []\n",
    "            paths_nodes = []\n",
    "            for _ in range(3):\n",
    "                trail, _ = landscape.extract_narrative(s, t, hidden_nodes=paths_nodes)\n",
    "                if trail:\n",
    "                    nt.append(trail)\n",
    "                    paths_nodes.extend(trail[1:-1])\n",
    "\n",
    "            # Narrative landscape with node closeness information\n",
    "            nt_cc = []\n",
    "            paths_nodes = []\n",
    "            for _ in range(3):\n",
    "                trail, _ = landscape_closeness.extract_narrative(s, t, hidden_nodes=paths_nodes)\n",
    "                if trail:\n",
    "                    nt_cc.append(trail)\n",
    "                    paths_nodes.extend(trail[1:-1])\n",
    "\n",
    "            all_narratives[(s, t, idx)] = {\n",
    "                \"random_points\": [\n",
    "                    [s, *np.random.randint(0, len(data), np.random.randint(5, 18)), t],\n",
    "                    [s, *np.random.randint(0, len(data), np.random.randint(5, 18)), t],\n",
    "                    [s, *np.random.randint(0, len(data), np.random.randint(5, 18)), t]\n",
    "                ],\n",
    "                \"shortest_path\": sp,\n",
    "                \"narrative_trails\": nt,\n",
    "                \"narrative_trails_closeness\": nt_cc,\n",
    "            }\n",
    "    with open('./extracted_narratives.pkl', 'wb') as handle:\n",
    "        pickle.dump(all_narratives, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_comparison_counts = {\"@1\": 0, \"@2\": 0, \"@3\": 0}\n",
    "\n",
    "\n",
    "def compute_absolute_metrics(kind):\n",
    "    results = []  # (method, k, value)\n",
    "\n",
    "    # Note that for metrics on the narratives with closeness centrality information,\n",
    "    # we still use the original base coherence to keep all results in the same scale.\n",
    "    for (_, _, story_idx), narratives in tqdm(all_narratives.items()):\n",
    "        true_storyline = ground_truths[\"path\"].iloc[story_idx].split(\";\")\n",
    "        true_storyline = [ordered_data_titles.index(unquote(x)) for x in true_storyline]\n",
    "\n",
    "        rp = narratives[f\"random_points\"]\n",
    "        sp = narratives[f\"shortest_path\"]\n",
    "        nt = narratives[f\"narrative_trails\"]\n",
    "        cc = narratives[f\"narrative_trails_closeness\"]\n",
    "        \n",
    "        wikispeedia = absolute_metrics(true_storyline, landscape, kind)\n",
    "        res_rp_k1 = absolute_metrics(rp[0], landscape, kind)\n",
    "        res_sp_k1 = absolute_metrics(sp[0], landscape, kind)\n",
    "        res_nt_k1 = absolute_metrics(nt[0], landscape, kind)\n",
    "        res_rt_k1 = absolute_metrics(Storyline(landscape, nt[0]).reduce_redundancy(), landscape, kind)\n",
    "        res_cc_k1 = absolute_metrics(cc[0], landscape, kind)\n",
    "        res_rc_k1 = absolute_metrics(Storyline(landscape, cc[0]).reduce_redundancy(), landscape, kind)\n",
    "\n",
    "        results.append((\"Wikispeedia\", 1, wikispeedia))\n",
    "        results.append((\"Random Points\", 1, res_rp_k1))\n",
    "        results.append((\"Shortest Path\", 1, res_sp_k1))\n",
    "        results.append((\"Narrative Trails\", 1, res_nt_k1))\n",
    "        results.append((\"Reduced Trails\", 1, res_rt_k1))\n",
    "        results.append((\"Narrative Trails (CC)\", 1, res_cc_k1))\n",
    "        results.append((\"Reduced Trails (CC)\", 1, res_rc_k1))\n",
    "\n",
    "        abs_comparison_counts[\"@1\"] += 1\n",
    "\n",
    "        if len(rp) >= 2 and len(sp) >= 2 and len(nt) >= 2 and len(cc) >= 2:\n",
    "            res_rp_k2 = absolute_metrics(rp[1], landscape, kind)\n",
    "            res_sp_k2 = absolute_metrics(sp[1], landscape, kind)\n",
    "            res_nt_k2 = absolute_metrics(nt[1], landscape, kind)\n",
    "            res_rt_k2 = absolute_metrics(Storyline(landscape, nt[1]).reduce_redundancy(), landscape, kind)\n",
    "            res_cc_k2 = absolute_metrics(cc[1], landscape, kind)\n",
    "            res_rc_k2 = absolute_metrics(Storyline(landscape, cc[1]).reduce_redundancy(), landscape, kind)\n",
    "\n",
    "            results.append((\"Random Points\", 2, sum((res_rp_k1, res_rp_k2)) / 2))\n",
    "            results.append((\"Shortest Path\", 2, sum((res_sp_k1, res_sp_k2)) / 2))\n",
    "            results.append((\"Narrative Trails\", 2, sum((res_nt_k1, res_nt_k2)) / 2))\n",
    "            results.append((\"Reduced Trails\", 2, sum((res_rt_k1, res_rt_k2)) / 2))\n",
    "            results.append((\"Narrative Trails (CC)\", 2, sum((res_cc_k1, res_cc_k2)) / 2))\n",
    "            results.append((\"Reduced Trails (CC)\", 2, sum((res_rc_k1, res_rc_k2)) / 2))\n",
    "\n",
    "            abs_comparison_counts[\"@2\"] += 1\n",
    "\n",
    "        if len(rp) == 3 and len(sp) == 3 and len(nt) == 3 and len(cc) == 3:\n",
    "            res_rp_k3 = absolute_metrics(rp[2], landscape, kind)\n",
    "            res_sp_k3 = absolute_metrics(sp[2], landscape, kind)\n",
    "            res_nt_k3 = absolute_metrics(nt[2], landscape, kind)\n",
    "            res_rt_k3 = absolute_metrics(Storyline(landscape, nt[2]).reduce_redundancy(), landscape, kind)\n",
    "            res_cc_k3 = absolute_metrics(cc[2], landscape, kind)\n",
    "            res_rc_k3 = absolute_metrics(Storyline(landscape, cc[2]).reduce_redundancy(), landscape, kind)\n",
    "\n",
    "            results.append((\"Random Points\", 3, sum((res_rp_k1, res_rp_k2, res_rp_k3)) / 3))\n",
    "            results.append((\"Shortest Path\", 3, sum((res_sp_k1, res_sp_k2, res_sp_k3)) / 3))\n",
    "            results.append((\"Narrative Trails\", 3, sum((res_nt_k1, res_nt_k2, res_nt_k3)) / 3))\n",
    "            results.append((\"Reduced Trails\", 3, sum((res_rt_k1, res_rt_k2, res_rt_k3)) / 3))\n",
    "            results.append((\"Narrative Trails (CC)\", 3, sum((res_cc_k1, res_cc_k2, res_cc_k3)) / 3))\n",
    "            results.append((\"Reduced Trails (CC)\", 3, sum((res_rc_k1, res_rc_k2, res_rc_k3)) / 3))\n",
    "\n",
    "            abs_comparison_counts[\"@3\"] += 1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_comparison_counts = {\"@1\": 0, \"@2\": 0, \"@3\": 0}\n",
    "\n",
    "\n",
    "def compute_metric_with_ground_truth(metric, low_emb):\n",
    "    results = []  # (method, k, value)\n",
    "\n",
    "    for (_, _, story_idx), narratives in tqdm(all_narratives.items()):\n",
    "        true_storyline = ground_truths[\"path\"].iloc[story_idx].split(\";\")\n",
    "        true_storyline = [ordered_data_titles.index(unquote(x)) for x in true_storyline]\n",
    "\n",
    "        rp = narratives[f\"random_points\"]\n",
    "        sp = narratives[f\"shortest_path\"]\n",
    "        nt = narratives[f\"narrative_trails\"]\n",
    "        cc = narratives[f\"narrative_trails_closeness\"]\n",
    "\n",
    "        res_rp_k1 = metric(true_storyline, rp[0], low_emb)\n",
    "        res_sp_k1 = metric(true_storyline, sp[0], low_emb)\n",
    "        res_nt_k1 = metric(true_storyline, nt[0], low_emb)\n",
    "        nt_st_line = Storyline(landscape, nt[0])\n",
    "        res_rt_k1 = metric(true_storyline, nt_st_line.reduce_redundancy(), low_emb)\n",
    "        res_cc_k1 = metric(true_storyline, cc[0], low_emb)\n",
    "        nt_st_line = Storyline(landscape_closeness, cc[0])\n",
    "        res_rc_k1 = metric(true_storyline, nt_st_line.reduce_redundancy(), low_emb)\n",
    "\n",
    "        results.append((\"Random Points\", 1, res_rp_k1))\n",
    "        results.append((\"Shortest Path\", 1, res_sp_k1))\n",
    "        results.append((\"Narrative Trails\", 1, res_nt_k1))\n",
    "        results.append((\"Reduced Trails\", 1, res_rt_k1))\n",
    "        results.append((\"Narrative Trails (CC)\", 1, res_cc_k1))\n",
    "        results.append((\"Reduced Trails (CC)\", 1, res_rc_k1))\n",
    "\n",
    "        gt_comparison_counts[\"@1\"] += 1\n",
    "\n",
    "        if len(rp) >= 2 and len(sp) >= 2 and len(nt) >= 2 and len(cc) >= 2:\n",
    "            res_rp_k2 = metric(true_storyline, rp[1], low_emb)\n",
    "            res_sp_k2 = metric(true_storyline, sp[1], low_emb)\n",
    "            res_nt_k2 = metric(true_storyline, nt[1], low_emb)\n",
    "            nt_st_line = Storyline(landscape, nt[1])\n",
    "            res_rt_k2 = metric(true_storyline, nt_st_line.reduce_redundancy(), low_emb)\n",
    "            res_cc_k2 = metric(true_storyline, cc[1], low_emb)\n",
    "            rc_st_line = Storyline(landscape_closeness, cc[1])\n",
    "            res_rc_k2 = metric(true_storyline, rc_st_line.reduce_redundancy(), low_emb)\n",
    "\n",
    "            results.append((\"Random Points\", 2, sum((res_rp_k1, res_rp_k2)) / 2))\n",
    "            results.append((\"Shortest Path\", 2, sum((res_sp_k1, res_sp_k2)) / 2))\n",
    "            results.append((\"Narrative Trails\", 2, sum((res_nt_k1, res_nt_k2)) / 2))\n",
    "            results.append((\"Reduced Trails\", 2, sum((res_rt_k1, res_rt_k2)) / 2))\n",
    "            results.append((\"Narrative Trails (CC)\", 2, sum((res_cc_k1, res_cc_k2)) / 2))\n",
    "            results.append((\"Reduced Trails (CC)\", 2, sum((res_rc_k1, res_rc_k2)) / 2))\n",
    "\n",
    "            gt_comparison_counts[\"@2\"] += 1\n",
    "\n",
    "        if len(rp) == 3 and len(sp) == 3 and len(nt) == 3 and len(cc) == 3:\n",
    "            res_rp_k3 = metric(true_storyline, rp[2], low_emb)\n",
    "            res_sp_k3 = metric(true_storyline, sp[2], low_emb)\n",
    "            res_nt_k3 = metric(true_storyline, nt[2], low_emb)\n",
    "            nt_st_line = Storyline(landscape, nt[2])\n",
    "            res_rt_k3 = metric(true_storyline, nt_st_line.reduce_redundancy(), low_emb)\n",
    "            res_cc_k3 = metric(true_storyline, cc[2], low_emb)\n",
    "            rc_st_line = Storyline(landscape_closeness, cc[2])\n",
    "            res_rc_k3 = metric(true_storyline, rc_st_line.reduce_redundancy(), low_emb)\n",
    "\n",
    "            results.append((\"Random Points\", 3, sum((res_rp_k1, res_rp_k2, res_rp_k3)) / 3))\n",
    "            results.append((\"Shortest Path\", 3, sum((res_sp_k1, res_sp_k2, res_sp_k3)) / 3))\n",
    "            results.append((\"Narrative Trails\", 3, sum((res_nt_k1, res_nt_k2, res_nt_k3)) / 3))\n",
    "            results.append((\"Reduced Trails\", 3, sum((res_rt_k1, res_rt_k2, res_rt_k3)) / 3))\n",
    "            results.append((\"Narrative Trails (CC)\", 3, sum((res_cc_k1, res_cc_k2, res_cc_k3)) / 3))\n",
    "            results.append((\"Reduced Trails (CC)\", 3, sum((res_rc_k1, res_rc_k2, res_rc_k3)) / 3))\n",
    "\n",
    "            gt_comparison_counts[\"@3\"] += 1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10607/10607 [00:03<00:00, 2883.82it/s]\n",
      "100%|██████████| 10607/10607 [00:03<00:00, 3097.65it/s]\n",
      "100%|██████████| 10607/10607 [00:40<00:00, 264.41it/s]\n",
      "100%|██████████| 10607/10607 [00:23<00:00, 460.88it/s]\n"
     ]
    }
   ],
   "source": [
    "df_rel = pd.DataFrame(compute_absolute_metrics(\"reliability\"), columns=[\"Method\", \"k\", \"Rel\"])\n",
    "df_rel = df_rel.groupby(by=[\"Method\", \"k\"]).mean()\n",
    "\n",
    "df_coh = pd.DataFrame(compute_absolute_metrics(\"min_coherence\"), columns=[\"Method\", \"k\", \"Coh\"])\n",
    "df_coh = df_coh.groupby(by=[\"Method\", \"k\"]).mean()\n",
    "\n",
    "gt_sim_res = compute_metric_with_ground_truth(similarity_metric, landscape.low_dim_embeds)\n",
    "df_sim = pd.DataFrame(gt_sim_res, columns=[\"Method\", \"k\", \"Sim\"]).groupby(by=[\"Method\", \"k\"]).mean()\n",
    "\n",
    "gt_dist_res = compute_metric_with_ground_truth(dtw_metric, landscape.low_dim_embeds)\n",
    "df_dtw = pd.DataFrame(gt_dist_res, columns=[\"Method\", \"k\", \"DTW\"]).groupby(by=[\"Method\", \"k\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10607 of 10832\n",
      "{'@1': 21214, '@2': 19172, '@3': 16770}\n",
      "{'@1': 21214, '@2': 19172, '@3': 16770}\n"
     ]
    }
   ],
   "source": [
    "print(len(all_narratives.items()), \"of\", len(ground_truths))\n",
    "print(gt_comparison_counts) # Divide by 2\n",
    "print(abs_comparison_counts) # Divide by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coh_1</th>\n",
       "      <th>Coh_2</th>\n",
       "      <th>Coh_3</th>\n",
       "      <th>Rel_1</th>\n",
       "      <th>Rel_2</th>\n",
       "      <th>Rel_3</th>\n",
       "      <th>Sim_1</th>\n",
       "      <th>Sim_2</th>\n",
       "      <th>Sim_3</th>\n",
       "      <th>DTW_1</th>\n",
       "      <th>DTW_2</th>\n",
       "      <th>DTW_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Wikispeedia</th>\n",
       "      <td>0.419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Points</th>\n",
       "      <td>0.320</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.347</td>\n",
       "      <td>2.200</td>\n",
       "      <td>2.201</td>\n",
       "      <td>2.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shortest Path</th>\n",
       "      <td>0.558</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Narrative Trails</th>\n",
       "      <td>0.709</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.787</td>\n",
       "      <td>1.029</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reduced Trails</th>\n",
       "      <td>0.668</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.771</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.077</td>\n",
       "      <td>1.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Narrative Trails (CC)</th>\n",
       "      <td>0.640</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.766</td>\n",
       "      <td>1.029</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reduced Trails (CC)</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.069</td>\n",
       "      <td>1.082</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Coh_1  Coh_2  Coh_3  Rel_1  Rel_2  Rel_3  Sim_1  Sim_2  \\\n",
       "Method                                                                          \n",
       "Wikispeedia            0.419    NaN    NaN  0.609    NaN    NaN    NaN    NaN   \n",
       "Random Points          0.320  0.321  0.322  0.454  0.455  0.456  0.347  0.347   \n",
       "Shortest Path          0.558  0.560  0.563  0.614  0.615  0.620  0.742  0.742   \n",
       "Narrative Trails       0.709  0.704  0.704  0.776  0.769  0.767  0.788  0.785   \n",
       "Reduced Trails         0.668  0.667  0.669  0.760  0.756  0.754  0.768  0.768   \n",
       "Narrative Trails (CC)  0.640  0.631  0.630  0.753  0.748  0.746  0.777  0.778   \n",
       "Reduced Trails (CC)    0.630  0.625  0.624  0.736  0.734  0.734  0.758  0.760   \n",
       "\n",
       "                       Sim_3  DTW_1  DTW_2  DTW_3  \n",
       "Method                                             \n",
       "Wikispeedia              NaN    NaN    NaN    NaN  \n",
       "Random Points          0.347  2.200  2.201  2.200  \n",
       "Shortest Path          0.746  0.967  0.978  0.971  \n",
       "Narrative Trails       0.787  1.029  1.049  1.063  \n",
       "Reduced Trails         0.771  1.056  1.077  1.088  \n",
       "Narrative Trails (CC)  0.766  1.029  1.049  1.093  \n",
       "Reduced Trails (CC)    0.750  1.069  1.082  1.119  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index\n",
    "df = pd.concat([df_rel, df_coh, df_dtw, df_sim], axis=1)\n",
    "df = df.reset_index().round(3)\n",
    "\n",
    "# Set the new index\n",
    "df = df.set_index(['Method'])\n",
    "\n",
    "# Melt the DataFrame\n",
    "df_melted = df.reset_index().melt(\n",
    "    id_vars=['Method', 'k'],\n",
    "    value_vars=['Rel', 'Coh', 'Sim', 'DTW'],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivot = df_melted.pivot_table(\n",
    "    index=['Method'],\n",
    "    columns=['metric', 'k'],\n",
    "    values='value'\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "df_pivot.columns = [\n",
    "    f\"{metric}_{k}\" for metric, k in df_pivot.columns\n",
    "]\n",
    "\n",
    "df_pivot = df_pivot[[\"Coh_1\", \"Coh_2\", \"Coh_3\", \"Rel_1\", \"Rel_2\", \"Rel_3\", \"Sim_1\", \"Sim_2\", \"Sim_3\", \"DTW_1\", \"DTW_2\", \"DTW_3\"]]\n",
    "df_pivot = df_pivot.loc[[\n",
    "    \"Wikispeedia\",\n",
    "    \"Random Points\",\n",
    "    \"Shortest Path\",\n",
    "    \"Narrative Trails\",\n",
    "    \"Reduced Trails\",\n",
    "    \"Narrative Trails (CC)\",\n",
    "    \"Reduced Trails (CC)\",\n",
    "]]\n",
    "\n",
    "df_pivot.to_csv(\"benchmark_results.csv\")\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliability: 0.6141220377905909\n",
      "Coherences: [0.67154048 0.66929064 0.63787183 0.58873993 0.55455038 0.57311691]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913     Cornell_University\n",
       "3668         United_States\n",
       "926                Country\n",
       "1101                 Earth\n",
       "3254         South_America\n",
       "286              Argentina\n",
       "624           Buenos_Aires\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_idx = pd.Series(all_narratives.keys()).sample(1, random_state=0).values[0]\n",
    "gt = [ordered_data_titles.index(unquote(x)) for x in ground_truths[\"path\"].iloc[story_idx[-1]].split(\";\")]\n",
    "\n",
    "print(\"Reliability:\", Storyline(landscape, gt).reliability())\n",
    "print(\"Coherences:\", Storyline(landscape, gt).path_base_coherence())\n",
    "\n",
    "data.iloc[gt][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliability: 0.666284836733653\n",
      "Coherences: [0.67469644 0.65797811]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913    Cornell_University\n",
       "464               Beijing\n",
       "624          Buenos_Aires\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = all_narratives[story_idx][\"shortest_path\"][0]\n",
    "print(\"Reliability:\", Storyline(landscape, sp).reliability())\n",
    "print(\"Coherences:\", Storyline(landscape, sp).path_base_coherence())\n",
    "\n",
    "data.iloc[sp][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliability: 0.7292108849616922\n",
      "Coherences: [0.70087677 0.72335243 0.72531052 0.76853098 0.71967434 0.7322392\n",
      " 0.71036678 0.7557472 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913       Cornell_University\n",
       "665                   Canada\n",
       "17           1973_oil_crisis\n",
       "268    Arab-Israeli_conflict\n",
       "269              Arab_League\n",
       "116            African_Union\n",
       "86                     Abuja\n",
       "586                   Brazil\n",
       "624             Buenos_Aires\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt = all_narratives[story_idx][\"narrative_trails\"][0]\n",
    "\n",
    "print(\"Reliability:\", Storyline(landscape, nt).reliability())\n",
    "print(\"Coherences:\", Storyline(landscape, nt).path_base_coherence())\n",
    "\n",
    "data.iloc[nt][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliability: 0.7292108849616922\n",
      "Coherences: [0.70087677 0.72335243 0.72531052 0.76853098 0.71967434 0.7322392\n",
      " 0.71036678 0.7557472 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913       Cornell_University\n",
       "665                   Canada\n",
       "17           1973_oil_crisis\n",
       "268    Arab-Israeli_conflict\n",
       "269              Arab_League\n",
       "116            African_Union\n",
       "86                     Abuja\n",
       "586                   Brazil\n",
       "624             Buenos_Aires\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_reduced = Storyline(landscape, nt).reduce_redundancy()\n",
    "\n",
    "print(\"Reliability:\", Storyline(landscape, nt_reduced).reliability())\n",
    "print(\"Coherences:\", Storyline(landscape, nt_reduced).path_base_coherence())\n",
    "\n",
    "data.iloc[nt_reduced][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-9b28e6b0e9dc41f0b4091819122f3ac5.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-9b28e6b0e9dc41f0b4091819122f3ac5.vega-embed details,\n",
       "  #altair-viz-9b28e6b0e9dc41f0b4091819122f3ac5.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-9b28e6b0e9dc41f0b4091819122f3ac5\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9b28e6b0e9dc41f0b4091819122f3ac5\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9b28e6b0e9dc41f0b4091819122f3ac5\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300, \"strokeWidth\": 0}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"blue\"}, \"encoding\": {\"x\": {\"axis\": null, \"field\": \"index\", \"type\": \"quantitative\"}, \"y\": {\"value\": 0}}}, {\"mark\": {\"type\": \"point\", \"color\": \"blue\"}, \"encoding\": {\"x\": {\"field\": \"index\", \"type\": \"quantitative\"}, \"y\": {\"value\": 0}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"angle\": 315, \"baseline\": \"bottom\", \"dx\": 2, \"dy\": -5}, \"encoding\": {\"text\": {\"field\": \"title\", \"type\": \"nominal\"}, \"x\": {\"field\": \"index\", \"type\": \"quantitative\"}, \"y\": {\"value\": 0}}}], \"data\": {\"name\": \"data-fb9892d961355878c4b6b6ad0c4eb075\"}, \"height\": 50, \"width\": 300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-fb9892d961355878c4b6b6ad0c4eb075\": [{\"title\": \"Cornell_University\", \"index\": 0}, {\"title\": \"Canada\", \"index\": 1}, {\"title\": \"1973_oil_crisis\", \"index\": 2}, {\"title\": \"Arab-Israeli_conflict\", \"index\": 3}, {\"title\": \"Arab_League\", \"index\": 4}, {\"title\": \"African_Union\", \"index\": 5}, {\"title\": \"Abuja\", \"index\": 6}, {\"title\": \"Brazil\", \"index\": 7}, {\"title\": \"Buenos_Aires\", \"index\": 8}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(data.iloc[nt_reduced][\"title\"])\n",
    "df['index'] = range(len(df))\n",
    "\n",
    "# Base chart: Line\n",
    "line = alt.Chart(df).mark_line(color='blue').encode(\n",
    "    x=alt.X('index', axis=None),\n",
    "    y=alt.value(0)\n",
    ")\n",
    "\n",
    "# Points\n",
    "points = alt.Chart(df).mark_point(color='blue').encode(\n",
    "    x='index',\n",
    "    y=alt.value(0)\n",
    ")\n",
    "\n",
    "# Text labels\n",
    "text = alt.Chart(df).mark_text(\n",
    "    align='left',\n",
    "    baseline='bottom',\n",
    "    dx=2,\n",
    "    dy=-5,\n",
    "    angle=315\n",
    ").encode(\n",
    "    x='index',\n",
    "    y=alt.value(0),\n",
    "    text='title'\n",
    ")\n",
    "\n",
    "# Combine charts\n",
    "chart = (line + points + text).configure_view(\n",
    "    strokeWidth=0\n",
    ").properties(\n",
    "    width=300,\n",
    "    height=50\n",
    ")\n",
    "\n",
    "# Display the chart\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrative-trails",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
