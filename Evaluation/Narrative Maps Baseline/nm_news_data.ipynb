{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ae241f",
   "metadata": {},
   "source": [
    "# Narrative Maps Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6fa888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# To make our relative library imports work\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f695cc",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b431581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustogerman/miniforge3/envs/narrative-trails/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "import math\n",
    "from math import log, exp, pi, sqrt, ceil\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "from pulp import *\n",
    "\n",
    "from Library.embedding_extraction import extract_embeddings\n",
    "\n",
    "from narrative_maps import (\n",
    "    extract_varsdict,\n",
    "    compute_temp_distance_table,\n",
    "    build_graph,\n",
    "    graph_stories,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67bcfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fddf4",
   "metadata": {},
   "source": [
    "## Linear Program Construction\n",
    "\n",
    "This has a lot of parameters, some of them ended up unused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa154a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LP(query, sim_table, membership_vectors, clust_sim_table, exp_temp_table, ent_table, numclust, relevance_table,\n",
    "              K, mincover, sigma_t, credibility=[], bias=[], operations=[],\n",
    "              has_start=True, has_end=False, window_time=None, cluster_list=[], start_nodes=[], end_nodes=[],\n",
    "              verbose=True, force_cluster=True, previous_varsdict=None):\n",
    "    n = len(query.index)  # We can cut out everything after the end.\n",
    "    # Variable names and indices\n",
    "    var_i = []\n",
    "    var_ij = []\n",
    "    var_k = [str(k) for k in range(0, numclust)]\n",
    "\n",
    "    for i in range(0, n):  # This goes up from 0 to n-1.\n",
    "        var_i.append(str(i))\n",
    "        for j in window_i_j[i]:\n",
    "            if i == j:\n",
    "                print(\"ERROR IN WINDOW - BASE\")\n",
    "            var_ij.append(str(i) + \"_\" + str(j))\n",
    "\n",
    "    # Linear program variable declaration.\n",
    "    minedge = LpVariable(\"minedge\", lowBound=0, upBound=1)\n",
    "    node_act_vars = LpVariable.dicts(\"node_act\", var_i, lowBound=0, upBound=1)\n",
    "    node_next_vars = LpVariable.dicts(\"node_next\", var_ij, lowBound=0,  upBound=1)\n",
    "    # clust_active_vars = LpVariable.dicts(\"clust_active\", var_k, lowBound=0, upBound=1)\n",
    "\n",
    "    # Create the 'prob' variable to contain the problem data\n",
    "    prob = LpProblem(\"StoryChainProblem\", LpMaximize)\n",
    "    # The objective function is added to 'prob' first\n",
    "    prob += minedge, \"WeakestLink\"\n",
    "\n",
    "    # Chain restrictions\n",
    "    if has_start:\n",
    "        num_starts = len(start_nodes)\n",
    "        if verbose:\n",
    "            print(\"Start node(s):\")\n",
    "            print(start_nodes)\n",
    "        if num_starts == 0:  # This is the default when no list is given and it has a start.\n",
    "            prob += node_act_vars[str(0)] == 1, 'InitialNode'\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Added start node(s)\")\n",
    "                print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "            initial_energy = 1.0 / num_starts\n",
    "            earliest_start = min(start_nodes)\n",
    "            for node in start_nodes:\n",
    "                prob += node_act_vars[str(node)] == initial_energy, 'InitialNode' + str(node)\n",
    "            for node in range(0, earliest_start):\n",
    "                prob += node_act_vars[str(node)] == 0, 'BeforeStart' + str(node)\n",
    "    if has_end:\n",
    "        num_ends = len(end_nodes)\n",
    "        if verbose:\n",
    "            print(\"End node(s):\")\n",
    "            print(end_nodes)\n",
    "        if num_ends == 0:  # This is the default when no list is given and it has a start.\n",
    "            prob += node_act_vars[str(n - 1)] == 1, 'FinalNode'\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Added end node(s)\")\n",
    "                print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "            final_energy = 1.0 / num_ends\n",
    "            latest_end = min(end_nodes)\n",
    "            for node in end_nodes:\n",
    "                prob += node_act_vars[str(node)] == final_energy, 'FinalNode' + str(node)\n",
    "            for node in range(latest_end + 1, n):\n",
    "                prob += node_act_vars[str(node)] == 0, 'AfterEnd' + str(node)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Chain constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    prob += lpSum([node_act_vars[i] for i in var_i]) == K, 'KNodes'\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Expected length constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if has_start:\n",
    "        if verbose:\n",
    "            print(\"Equality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for j in range(1, n):\n",
    "            if j not in start_nodes:\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for i in window_j_i[j]]) == node_act_vars[str(j)], 'InEdgeReq' + str(j)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Generating specific starting node constraints.\")\n",
    "                    print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for i in window_j_i[j]]) == 0, 'InEdgeReq' + str(j)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Inequality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for j in range(1, n):\n",
    "            prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                          for i in window_j_i[j]]) <= node_act_vars[str(j)], 'InEdgeReq' + str(j)\n",
    "    if verbose:\n",
    "        print(\"In-degree constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if has_end:\n",
    "        if verbose:\n",
    "            print(\"Equality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for i in range(0, n - 1):\n",
    "            if i not in end_nodes:\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for j in window_i_j[i]]) == node_act_vars[str(i)], 'OutEdgeReq' + str(i)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Generating specific starting node constraints.\")\n",
    "                    print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for j in window_i_j[i]]) == 0, 'OutEdgeReq' + str(i)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Inequality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for i in range(0, n - 1):\n",
    "            prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                          for j in window_i_j[i]]) <= node_act_vars[str(i)], 'OutEdgeReq' + str(i)\n",
    "    if verbose:\n",
    "        print(\"Out-degree constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # Objective\n",
    "    for i in range(0, n):\n",
    "        for j in window_i_j[i]:\n",
    "            coherence_weights = [0.5, 0.5]\n",
    "            # Five or more entities in common means double the connection strength.\n",
    "            entity_multiplier = min(1 + ent_table[i, j], 2)\n",
    "            # Geometric mean the relevances, multiply based on how far it is from 0.5.\n",
    "            relevance_multiplier = (relevance_table[i] * relevance_table[j]) ** 0.5\n",
    "            coherence = (sim_table[i, j] ** coherence_weights[0]) * \\\n",
    "                (clust_sim_table[i, j] ** coherence_weights[1])\n",
    "            weighted_coherence = min(coherence * entity_multiplier * relevance_multiplier, 1.0)\n",
    "            prob += minedge <= 1 - node_next_vars[str(i) + \"_\" + str(j)] + \\\n",
    "                weighted_coherence, \"Objective\" + str(i) + \"_\" + str(j)\n",
    "    if verbose:\n",
    "        print(\"Objective constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if previous_varsdict:\n",
    "        current_names = [v.name for v in prob.variables() if \"node_act\" in v.name]\n",
    "        if verbose:\n",
    "            print(\"Generated list of names.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for k, v in previous_varsdict.items():\n",
    "            if \"node_act\" in k and k in current_names:\n",
    "                node_act_vars[k.replace(\"node_act_\", \"\")].setInitialValue(v)\n",
    "\n",
    "    if verbose:\n",
    "        if previous_varsdict:\n",
    "            print(\"Used previous solution as starting point.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        else:\n",
    "            print(\"No previous solution available.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    # The problem data is written to an .lp file\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05d7ca",
   "metadata": {},
   "source": [
    "## Building the Graph Data Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920aa6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_df_multiple_starts(query, varsdict, prune=None, threshold=0.01, cluster_dict={}, start_nodes=[]):\n",
    "    n = len(query)\n",
    "    # This has some leftover stuff that is not really useful now.\n",
    "    if 'bias' in query.columns:\n",
    "        graph_df = pd.DataFrame(columns=['id', 'adj_list', 'adj_weights',\n",
    "                                'date', 'publication', 'title', 'text', 'url', 'bias', 'coherence'])\n",
    "    else:\n",
    "        graph_df = pd.DataFrame(columns=['id', 'adj_list', 'adj_weights',\n",
    "                                'date', 'publication', 'title', 'text', 'url', 'coherence'])\n",
    "\n",
    "    already_in = []\n",
    "    for i in range(0, n):\n",
    "        prob = []\n",
    "        coherence = varsdict[\"node_act_\" + str(i)]\n",
    "        if coherence <= threshold:\n",
    "            continue\n",
    "        coherence_list = []\n",
    "        index_list = []\n",
    "        for j in window_i_j[i]:\n",
    "            name = \"node_next_\" + str(i) + \"_\" + str(j)\n",
    "            prob.append(varsdict[name])\n",
    "            coherence_list.append(varsdict[\"node_act_\" + str(j)])\n",
    "        idx_list = [window_i_j[i][idx] for idx, e in enumerate(prob) if round(\n",
    "            e, 8) != 0 and e > threshold and coherence_list[idx] > threshold]  # idx + i + 1\n",
    "        nz_prob = [e for idx, e in enumerate(prob) if round(\n",
    "            e, 8) != 0 and e > threshold and coherence_list[idx] > threshold]\n",
    "        if prune:\n",
    "            if len(idx_list) > prune:\n",
    "                top_prob_idx = sorted(range(len(nz_prob)), key=lambda k: nz_prob[k])[-prune:]\n",
    "                idx_list = [idx_list[j] for j in top_prob_idx]\n",
    "                nz_prob = [nz_prob[idx] for idx in top_prob_idx]\n",
    "        sum_nz = sum(nz_prob)\n",
    "        nz_prob = [nz_prob[j] / sum_nz for j in range(0, len(nz_prob))]\n",
    "        # If we haven't checked this one before we add it to the graph.\n",
    "        url = str(query.iloc[i]['url'])\n",
    "        if i in already_in or sum_nz > 0:\n",
    "            if len(url) > 0:\n",
    "                url = urlparse(url).netloc\n",
    "            if not (graph_df['id'] == i).any():\n",
    "                title = query.iloc[i]['title']\n",
    "                for key, value in cluster_dict.items():\n",
    "                    if str(i) in value:\n",
    "                        title = \"[\" + str(key) + \"] \" + title\n",
    "                outgoing_edges = [idx_temp for idx_temp in idx_list]\n",
    "                # coherence = varsdict[\"node_act_\" + str(i)]\n",
    "                if 'bias' in query.columns:\n",
    "                    graph_df.loc[len(graph_df)] = [i, outgoing_edges, nz_prob, query.iloc[i]['date'], query.iloc[i]['publication'],\n",
    "                                                   title, '', query.iloc[i]['url'], query.iloc[i]['bias'], coherence]\n",
    "                else:\n",
    "                    graph_df.loc[len(graph_df)] = [i, outgoing_edges, nz_prob, query.iloc[i]['date'], query.iloc[i]['publication'],\n",
    "                                                   title, '', query.iloc[i]['url'], coherence]\n",
    "\n",
    "            already_in += [i] + idx_list\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b2a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = None\n",
    "window_i_j = {}\n",
    "window_j_i = {}\n",
    "\n",
    "\n",
    "def solve_LP(\n",
    "    query,\n",
    "    dataset,\n",
    "    membership_vectors,\n",
    "    K=6,\n",
    "    mincover=0.20,\n",
    "    sigma_t=30,\n",
    "    start_nodes=[],\n",
    "    end_nodes=[],\n",
    "    verbose=True,\n",
    "    force_cluster=True,\n",
    "    use_entities=True,\n",
    "    use_temporal=True,\n",
    "    strict_start=False,\n",
    "):\n",
    "\n",
    "    global start_time\n",
    "    start_time = time()\n",
    "\n",
    "    n = len(query.index)\n",
    "    # varsdict_filename = 'varsdict_' + dataset + \"_\" + str(n) + '.pickle'\n",
    "\n",
    "    if sigma_t != 0 and use_temporal:\n",
    "        exp_temp_table = np.exp(-temporal_distance_table / sigma_t)\n",
    "    else:\n",
    "        exp_temp_table = np.ones(temporal_distance_table.shape)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed temporal distance table.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    window_time = None\n",
    "    if sigma_t != 0 and use_temporal:\n",
    "        window_time = sigma_t * 3  # Days\n",
    "\n",
    "    if window_time is None:\n",
    "        for i in range(0, n):\n",
    "            window_i_j[i] = list(range(i + 1, n))\n",
    "        for j in range(0, n):\n",
    "            window_j_i[j] = list(range(0, j))\n",
    "    else:\n",
    "        for j in range(0, n):\n",
    "            window_j_i[j] = []\n",
    "        for i in range(0, n):\n",
    "            window_i_j[i] = []\n",
    "        for i in range(0, n - 1):\n",
    "            window = 0\n",
    "            for j in range(i + 1, n):\n",
    "                if temporal_distance_table[i, j] <= window_time:\n",
    "                    window += 1\n",
    "            window = max(min(5, n - i), window)\n",
    "            window_i_j[i] = list(range(i + 1, min(i + window, n)))\n",
    "            for j in window_i_j[i]:\n",
    "                window_j_i[j].append(i)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed temporal windows.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed entity similarities.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    ent_table = np.zeros((n, n))  # Fill entity information with zeros by default.\n",
    "    actual_ent_table = ent_table\n",
    "    ent_doc_list = None\n",
    "    if use_entities:\n",
    "        ent_table, ent_doc_list = get_entity_table(query, dataset)\n",
    "        actual_ent_table = ent_table\n",
    "\n",
    "    # Deprecated relevance table computation\n",
    "    relevance_table = [1.0] * membership_vectors.shape[0]  # Create a vector full of 1s\n",
    "\n",
    "    has_start = False\n",
    "    if start_nodes is not None:\n",
    "        has_start = (len(start_nodes) > 0)\n",
    "    if end_nodes is not None:\n",
    "        has_end = (len(end_nodes) > 0)\n",
    "    if verbose:\n",
    "        print(\"Creating LP...\")\n",
    "\n",
    "    # Read previous solution and feed to LP. If none there is no previous solution.\n",
    "    previous_varsdict = None\n",
    "    # if os.path.isfile(varsdict_filename):\n",
    "    #     with open(varsdict_filename, 'rb') as handle:\n",
    "    #         previous_varsdict = pickle.load(handle)\n",
    "\n",
    "    prob = create_LP(\n",
    "        query,\n",
    "        sim_table,\n",
    "        membership_vectors,\n",
    "        clust_sim_table,\n",
    "        exp_temp_table,\n",
    "        actual_ent_table,\n",
    "        numclust,\n",
    "        relevance_table,\n",
    "        K=K,\n",
    "        mincover=mincover,\n",
    "        sigma_t=sigma_t,\n",
    "        has_start=has_start,\n",
    "        has_end=has_end,\n",
    "        start_nodes=start_nodes,\n",
    "        end_nodes=end_nodes,\n",
    "        verbose=verbose,\n",
    "        force_cluster=force_cluster,\n",
    "        previous_varsdict=previous_varsdict\n",
    "    )\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(\"Saving model...\")\n",
    "    #     print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # prob.writeLP(\"left_story.lp\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Solving model...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # (GLPK_CMD(path = 'C:\\\\glpk-4.65\\\\w64\\\\glpsol.exe', options = [\"--tmlim\", \"180\"]))\n",
    "\n",
    "    prob.solve(PULP_CBC_CMD(mip=False, warmStart=True, msg=verbose))\n",
    "\n",
    "    varsdict = extract_varsdict(prob)\n",
    "\n",
    "    # Overwrite last solution.\n",
    "    # with open(varsdict_filename, 'wb') as handle:\n",
    "    #     pickle.dump(varsdict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    graph_df = build_graph_df_multiple_starts(query, varsdict, prune=ceil(\n",
    "        sqrt(K)), threshold=0.1/K, cluster_dict={})\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Graph data frame construction...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if strict_start and has_start:\n",
    "        graph_df = graph_clean_up(graph_df, start_nodes)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Graph clean up...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    return [graph_df, (numclust, LpStatus[prob.status]), sim_table, clust_sim_table, ent_table, ent_doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378d128",
   "metadata": {},
   "source": [
    "## MAIN: Calling the Map Construction Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c530c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../../data/NewsData/embed_data-gpt4.pickle' loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>publication</th>\n",
       "      <th>full_text</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. restores commercial flights to Cuba</td>\n",
       "      <td>https://www.cnn.com/2016/02/12/politics/u-s-to...</td>\n",
       "      <td>2016-02-12</td>\n",
       "      <td></td>\n",
       "      <td>President Barack Obama and first lady Michelle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.013048902153968811, -0.015582204796373844, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Obama announces Cuba visit</td>\n",
       "      <td>https://www.cnn.com/2016/02/17/politics/obama-...</td>\n",
       "      <td>2016-02-18</td>\n",
       "      <td></td>\n",
       "      <td>President Barack Obama and first lady Michelle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.029543746262788773, -0.030289117246866226, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White House sees Cuba visit as chance to conso...</td>\n",
       "      <td>https://www.washingtonpost.com/politics/white-...</td>\n",
       "      <td>2016-02-19</td>\n",
       "      <td></td>\n",
       "      <td>In the months since President Obama announced ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.04994184896349907, -0.014324406161904335, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Obama's Cuba trip guarantees another Castro cr...</td>\n",
       "      <td>https://nypost.com/2016/02/18/obamas-cuba-trip...</td>\n",
       "      <td>2016-02-19</td>\n",
       "      <td></td>\n",
       "      <td>President Obama says hes headed to Cuba next m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.03186975419521332, 0.005861963611096144, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dissidents hope for public recognition from Ob...</td>\n",
       "      <td>http://www.firstpost.com/world/dissidents-hope...</td>\n",
       "      <td>2016-02-19</td>\n",
       "      <td></td>\n",
       "      <td>HAVANA Ostracized by the government and mistru...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.04770871251821518, -0.030910532921552658, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Cuba: Disobeying Protest Ban to have Serious C...</td>\n",
       "      <td>https://havanatimes.org/news/cuba-disobeying-p...</td>\n",
       "      <td>2021-10-21</td>\n",
       "      <td></td>\n",
       "      <td>The Cuban Attorney Generals Office issues its ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.05380697548389435, 0.014619889669120312, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Report: Cuba Engaged in 'Brutal Abuses' Agains...</td>\n",
       "      <td>https://www.breitbart.com/politics/2021/10/21/...</td>\n",
       "      <td>2021-10-21</td>\n",
       "      <td></td>\n",
       "      <td>Cuban officials are committing \"brutal abuses\"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.023437807336449623, 0.016891997307538986, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Cuba-U.S. tensions mount over pending protests...</td>\n",
       "      <td>https://www.reuters.com/world/americas/cuba-us...</td>\n",
       "      <td>2021-10-25</td>\n",
       "      <td></td>\n",
       "      <td>A vintage car passes by the U.S. Embassy carry...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.052503641694784164, -0.02012248896062374, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Cuba: Communists Display Weapons for 'People's...</td>\n",
       "      <td>https://www.breitbart.com/latin-america/2021/1...</td>\n",
       "      <td>2021-10-26</td>\n",
       "      <td></td>\n",
       "      <td>Cuban communists flooded social media outlets ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.0667709931731224, 0.04038030654191971, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>Cuba approves laws granting greater rights as ...</td>\n",
       "      <td>https://www.reuters.com/world/americas/cuba-ap...</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td></td>\n",
       "      <td>People shout slogans against the government du...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.03743836283683777, -0.0003174716839566827, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0             U.S. restores commercial flights to Cuba   \n",
       "1                           Obama announces Cuba visit   \n",
       "2    White House sees Cuba visit as chance to conso...   \n",
       "3    Obama's Cuba trip guarantees another Castro cr...   \n",
       "4    Dissidents hope for public recognition from Ob...   \n",
       "..                                                 ...   \n",
       "535  Cuba: Disobeying Protest Ban to have Serious C...   \n",
       "536  Report: Cuba Engaged in 'Brutal Abuses' Agains...   \n",
       "537  Cuba-U.S. tensions mount over pending protests...   \n",
       "538  Cuba: Communists Display Weapons for 'People's...   \n",
       "539  Cuba approves laws granting greater rights as ...   \n",
       "\n",
       "                                                   url       date publication  \\\n",
       "0    https://www.cnn.com/2016/02/12/politics/u-s-to... 2016-02-12               \n",
       "1    https://www.cnn.com/2016/02/17/politics/obama-... 2016-02-18               \n",
       "2    https://www.washingtonpost.com/politics/white-... 2016-02-19               \n",
       "3    https://nypost.com/2016/02/18/obamas-cuba-trip... 2016-02-19               \n",
       "4    http://www.firstpost.com/world/dissidents-hope... 2016-02-19               \n",
       "..                                                 ...        ...         ...   \n",
       "535  https://havanatimes.org/news/cuba-disobeying-p... 2021-10-21               \n",
       "536  https://www.breitbart.com/politics/2021/10/21/... 2021-10-21               \n",
       "537  https://www.reuters.com/world/americas/cuba-us... 2021-10-25               \n",
       "538  https://www.breitbart.com/latin-america/2021/1... 2021-10-26               \n",
       "539  https://www.reuters.com/world/americas/cuba-ap... 2021-10-28               \n",
       "\n",
       "                                             full_text  Unnamed: 5  \\\n",
       "0    President Barack Obama and first lady Michelle...         NaN   \n",
       "1    President Barack Obama and first lady Michelle...         NaN   \n",
       "2    In the months since President Obama announced ...         NaN   \n",
       "3    President Obama says hes headed to Cuba next m...         NaN   \n",
       "4    HAVANA Ostracized by the government and mistru...         NaN   \n",
       "..                                                 ...         ...   \n",
       "535  The Cuban Attorney Generals Office issues its ...         NaN   \n",
       "536  Cuban officials are committing \"brutal abuses\"...         NaN   \n",
       "537  A vintage car passes by the U.S. Embassy carry...         NaN   \n",
       "538  Cuban communists flooded social media outlets ...         NaN   \n",
       "539  People shout slogans against the government du...         NaN   \n",
       "\n",
       "                                                 embed  \n",
       "0    [0.013048902153968811, -0.015582204796373844, ...  \n",
       "1    [0.029543746262788773, -0.030289117246866226, ...  \n",
       "2    [0.04994184896349907, -0.014324406161904335, -...  \n",
       "3    [0.03186975419521332, 0.005861963611096144, 0....  \n",
       "4    [0.04770871251821518, -0.030910532921552658, 0...  \n",
       "..                                                 ...  \n",
       "535  [0.05380697548389435, 0.014619889669120312, 0....  \n",
       "536  [0.023437807336449623, 0.016891997307538986, 0...  \n",
       "537  [0.052503641694784164, -0.02012248896062374, 0...  \n",
       "538  [0.0667709931731224, 0.04038030654191971, 0.02...  \n",
       "539  [0.03743836283683777, -0.0003174716839566827, ...  \n",
       "\n",
       "[540 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news_articles = pd.read_csv(f\"../../data/NewsData/text_data.csv\")\n",
    "data_news_articles[\"date\"] = pd.to_datetime(data_news_articles[\"date\"], format=\"%m/%d/%y\")\n",
    "\n",
    "\n",
    "# Load embeddings for news articles\n",
    "news_data_embeds, _, _ = extract_embeddings(\n",
    "    text=data_news_articles[\"full_text\"].tolist(),\n",
    "    foldername=f\"../../data/NewsData\",\n",
    "    model_name=\"gpt4\"\n",
    ")\n",
    "\n",
    "data_news_articles[\"embed\"] = news_data_embeds.tolist()\n",
    "data_news_articles[\"publication\"] = \"\"\n",
    "data_news_articles.sort_values(by=\"date\").reset_index(names=\"idx\")\n",
    "\n",
    "# select sources and targets\n",
    "news_data_src = np.random.choice(data_news_articles.index[:(len(data_news_articles) // 2) - 50], 50)\n",
    "news_data_tgt = np.random.choice(data_news_articles.index[(len(data_news_articles) // 2) + 50:], 50)\n",
    "\n",
    "data_news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5193cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 Topics\n"
     ]
    }
   ],
   "source": [
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=32,\n",
    "    n_components=48,\n",
    "    min_dist=0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    n_jobs=1,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "low_dim_mapper = umap_model.fit(np.array(data_news_articles[\"embed\"].tolist()))\n",
    "low_dim_embeds = low_dim_mapper.embedding_\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ").fit(low_dim_embeds)\n",
    "\n",
    "cluster_label_probs = hdbscan.prediction.all_points_membership_vectors(\n",
    "    hdbscan_model\n",
    ")\n",
    "\n",
    "cluster_labels = cluster_label_probs.argmax(1)\n",
    "\n",
    "data_news_articles[\"topic\"] = cluster_labels\n",
    "print(f\"Found {len(np.unique(cluster_labels))} Topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29150ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Length (usually values from 6 to 12 produce decent maps, but it depends on data set size and probably the underlying distribution of similarities).\n",
    "k_input = 12\n",
    "\n",
    "# % of average coverage we require. For small data sets 50-80 is good. For bigger data sets with many clusters you will likely only get 20%.\n",
    "# This was tested with values up to 500. After that I'm not sure how well the model will perform.\n",
    "mincover_input = 0\n",
    "\n",
    "# Temporal distance penalty in DAYS. I left it on 30 as default for the Cuban data set.\n",
    "# Lower values allow more temporally distant connections. Consider temporal density of the data when adjusting.\n",
    "# Can set it to 0 and it will be discarded from the computation.\n",
    "sigma_t = 0\n",
    "use_temporal = False  # Use this to enable or disable the temporal penalty, by default it is on.\n",
    "\n",
    "# Leave this as false, there was supposed to be a reward factor for events with common entities, but it adds too much computational time so not worth it.\n",
    "use_entities = False\n",
    "\n",
    "# If you enable strict start you will discard any storyline that does not start from the user-defined start node.\n",
    "# It is recommended to disable this to allow for extra storylines that emerge from the LP solution.\n",
    "strict_start = False\n",
    "\n",
    "# Compute angular similarity\n",
    "similarities = np.clip(cosine_similarity(np.array(data_news_articles[\"embed\"].tolist())), -1, 1)\n",
    "sim_table = (1 - np.arccos(similarities) / pi)\n",
    "mask = np.ones(sim_table.shape, dtype=bool)\n",
    "np.fill_diagonal(mask, 0)\n",
    "max_value = sim_table[mask].max()\n",
    "min_value = sim_table[mask].min()\n",
    "sim_table = (sim_table - min_value) / (max_value - min_value)\n",
    "sim_table = np.clip(sim_table, 0, 1)\n",
    "\n",
    "# Compute topic similarity\n",
    "numclust = 1\n",
    "clust_sim = np.zeros((cluster_label_probs.shape[0], cluster_label_probs.shape[0]))\n",
    "\n",
    "if len(cluster_label_probs.shape) > 1:\n",
    "    numclust = cluster_label_probs.shape[1]\n",
    "    cluster_label_probs[cluster_label_probs < 1/numclust] = 0\n",
    "    cluster_label_probs[np.all(cluster_label_probs == 0,\n",
    "                                        axis=1)] = np.ones(numclust) / numclust\n",
    "    row_sums = cluster_label_probs.sum(axis=1)\n",
    "    cluster_label_probs = cluster_label_probs / row_sums[:, np.newaxis]\n",
    "\n",
    "    clust_sim = distance.cdist(\n",
    "        cluster_label_probs,\n",
    "        cluster_label_probs,\n",
    "        lambda u, v: distance.jensenshannon(u, v, base=2.0)\n",
    "    )\n",
    "else:\n",
    "    cluster_label_probs = np.ones((cluster_label_probs.shape[0], 1))\n",
    "\n",
    "clust_sim_table = 1 - clust_sim\n",
    "\n",
    "# Compute temporal distance\n",
    "temporal_distance_table = compute_temp_distance_table(data_news_articles, \"./narrative_maps/temp/news_articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78300ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [29:27<00:00, 35.35s/it]\n"
     ]
    }
   ],
   "source": [
    "results_data = {\n",
    "    \"algorithm\": [],\n",
    "    \"src\": [],\n",
    "    \"tgt\": [],\n",
    "    \"exec_time\": [],\n",
    "    \"effective_exec_time\": [],\n",
    "    \"main_storyline\": [],\n",
    "    \"storylines\": []\n",
    "}\n",
    "\n",
    "for src, tgt in tqdm(zip(news_data_src, news_data_tgt), total=len(news_data_src)):\n",
    "    for i in range(4):\n",
    "        start_time = time()\n",
    "        graph_df_new, status, _, _, _, _ = solve_LP(\n",
    "            data_news_articles,\n",
    "            dataset=\"news_articles\",\n",
    "            membership_vectors=cluster_label_probs,\n",
    "            K=k_input,\n",
    "            mincover=mincover_input/100,\n",
    "            sigma_t=sigma_t,\n",
    "            start_nodes=[src],\n",
    "            end_nodes=[tgt],\n",
    "            verbose=False,\n",
    "            use_entities=use_entities,\n",
    "            use_temporal=use_temporal,\n",
    "            strict_start=strict_start,\n",
    "        )\n",
    "        end_time = time() - start_time\n",
    "\n",
    "        # Post Processing\n",
    "        if 'Optimal' in status[1]:\n",
    "            G = build_graph(graph_df_new)\n",
    "            storylines = graph_stories(G, start_nodes=[src], end_nodes=[tgt])\n",
    "\n",
    "            results_data[\"algorithm\"].append(\"narrative_maps\")\n",
    "            results_data[\"src\"].append(src)\n",
    "            results_data[\"tgt\"].append(tgt)\n",
    "            results_data[\"exec_time\"].append(end_time)\n",
    "            results_data[\"effective_exec_time\"].append(end_time / len(storylines))\n",
    "            results_data[\"main_storyline\"].append(storylines[0])\n",
    "            results_data[\"storylines\"].append(storylines)\n",
    "        else:\n",
    "            print(f\"** Warning: Experiment '({src}, {tgt})' not optimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644f252f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>exec_time</th>\n",
       "      <th>effective_exec_time</th>\n",
       "      <th>main_storyline</th>\n",
       "      <th>storylines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>23</td>\n",
       "      <td>393</td>\n",
       "      <td>9.199394</td>\n",
       "      <td>1.022155</td>\n",
       "      <td>[23, 45, 74, 182, 183, 260, 292, 315, 336, 371...</td>\n",
       "      <td>[[23, 45, 74, 182, 183, 260, 292, 315, 336, 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>187</td>\n",
       "      <td>324</td>\n",
       "      <td>8.302038</td>\n",
       "      <td>1.383673</td>\n",
       "      <td>[187, 215, 217, 221, 226, 230, 258, 261, 267, ...</td>\n",
       "      <td>[[187, 215, 217, 221, 226, 230, 258, 261, 267,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>182</td>\n",
       "      <td>512</td>\n",
       "      <td>8.491624</td>\n",
       "      <td>0.943514</td>\n",
       "      <td>[182, 206, 267, 328, 381, 383, 392, 512]</td>\n",
       "      <td>[[182, 206, 267, 328, 381, 383, 392, 512], [19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>88</td>\n",
       "      <td>386</td>\n",
       "      <td>8.327072</td>\n",
       "      <td>0.693923</td>\n",
       "      <td>[88, 90, 100, 103, 182, 196, 197, 247, 266, 32...</td>\n",
       "      <td>[[88, 90, 100, 103, 182, 196, 197, 247, 266, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>124</td>\n",
       "      <td>447</td>\n",
       "      <td>8.309196</td>\n",
       "      <td>1.187028</td>\n",
       "      <td>[124, 130, 179, 190, 209, 249, 250, 276, 323, ...</td>\n",
       "      <td>[[124, 130, 179, 190, 209, 249, 250, 276, 323,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>98</td>\n",
       "      <td>505</td>\n",
       "      <td>10.578361</td>\n",
       "      <td>0.961669</td>\n",
       "      <td>[98, 130, 476, 478, 479, 505]</td>\n",
       "      <td>[[98, 130, 476, 478, 479, 505], [112, 196, 244...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>88</td>\n",
       "      <td>386</td>\n",
       "      <td>8.358025</td>\n",
       "      <td>0.696502</td>\n",
       "      <td>[88, 90, 100, 103, 182, 196, 197, 247, 266, 32...</td>\n",
       "      <td>[[88, 90, 100, 103, 182, 196, 197, 247, 266, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>196</td>\n",
       "      <td>457</td>\n",
       "      <td>8.574661</td>\n",
       "      <td>1.714932</td>\n",
       "      <td>[196, 251, 252, 320, 322, 342, 348, 397, 446, ...</td>\n",
       "      <td>[[196, 251, 252, 320, 322, 342, 348, 397, 446,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>90</td>\n",
       "      <td>387</td>\n",
       "      <td>8.449609</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>[90, 91, 182, 223, 250, 289, 293, 386, 387]</td>\n",
       "      <td>[[90, 91, 182, 223, 250, 289, 293, 386, 387], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>184</td>\n",
       "      <td>364</td>\n",
       "      <td>8.321721</td>\n",
       "      <td>0.832172</td>\n",
       "      <td>[184, 185, 201, 260, 314, 364]</td>\n",
       "      <td>[[184, 185, 201, 260, 314, 364], [186], [206, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          algorithm  src  tgt  exec_time  effective_exec_time  \\\n",
       "7    narrative_maps   23  393   9.199394             1.022155   \n",
       "158  narrative_maps  187  324   8.302038             1.383673   \n",
       "155  narrative_maps  182  512   8.491624             0.943514   \n",
       "166  narrative_maps   88  386   8.327072             0.693923   \n",
       "136  narrative_maps  124  447   8.309196             1.187028   \n",
       "180  narrative_maps   98  505  10.578361             0.961669   \n",
       "164  narrative_maps   88  386   8.358025             0.696502   \n",
       "199  narrative_maps  196  457   8.574661             1.714932   \n",
       "81   narrative_maps   90  387   8.449609             0.844961   \n",
       "103  narrative_maps  184  364   8.321721             0.832172   \n",
       "\n",
       "                                        main_storyline  \\\n",
       "7    [23, 45, 74, 182, 183, 260, 292, 315, 336, 371...   \n",
       "158  [187, 215, 217, 221, 226, 230, 258, 261, 267, ...   \n",
       "155           [182, 206, 267, 328, 381, 383, 392, 512]   \n",
       "166  [88, 90, 100, 103, 182, 196, 197, 247, 266, 32...   \n",
       "136  [124, 130, 179, 190, 209, 249, 250, 276, 323, ...   \n",
       "180                      [98, 130, 476, 478, 479, 505]   \n",
       "164  [88, 90, 100, 103, 182, 196, 197, 247, 266, 32...   \n",
       "199  [196, 251, 252, 320, 322, 342, 348, 397, 446, ...   \n",
       "81         [90, 91, 182, 223, 250, 289, 293, 386, 387]   \n",
       "103                     [184, 185, 201, 260, 314, 364]   \n",
       "\n",
       "                                            storylines  \n",
       "7    [[23, 45, 74, 182, 183, 260, 292, 315, 336, 37...  \n",
       "158  [[187, 215, 217, 221, 226, 230, 258, 261, 267,...  \n",
       "155  [[182, 206, 267, 328, 381, 383, 392, 512], [19...  \n",
       "166  [[88, 90, 100, 103, 182, 196, 197, 247, 266, 3...  \n",
       "136  [[124, 130, 179, 190, 209, 249, 250, 276, 323,...  \n",
       "180  [[98, 130, 476, 478, 479, 505], [112, 196, 244...  \n",
       "164  [[88, 90, 100, 103, 182, 196, 197, 247, 266, 3...  \n",
       "199  [[196, 251, 252, 320, 322, 342, 348, 397, 446,...  \n",
       "81   [[90, 91, 182, 223, 250, 289, 293, 386, 387], ...  \n",
       "103  [[184, 185, 201, 260, 314, 364], [186], [206, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_data_df = pd.DataFrame(results_data)\n",
    "results_data_df.sample(10)  # Show 10 example results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8273d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data_df.to_pickle(\"./narrative_maps/results/news_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9add7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrative-trails",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
