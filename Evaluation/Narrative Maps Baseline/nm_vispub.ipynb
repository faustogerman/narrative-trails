{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ae241f",
   "metadata": {},
   "source": [
    "# Narrative Maps Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6fa888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# To make our relative library imports work\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f695cc",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b431581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustogerman/miniforge3/envs/narrative-trails/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "import math\n",
    "from math import log, exp, pi, sqrt, ceil\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "from pulp import *\n",
    "\n",
    "from Library.embedding_extraction import extract_embeddings\n",
    "\n",
    "from narrative_maps import (\n",
    "    extract_varsdict,\n",
    "    compute_temp_distance_table,\n",
    "    build_graph,\n",
    "    graph_stories,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67bcfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fddf4",
   "metadata": {},
   "source": [
    "## Linear Program Construction\n",
    "\n",
    "This has a lot of parameters, some of them ended up unused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa154a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LP(query, sim_table, membership_vectors, clust_sim_table, exp_temp_table, ent_table, numclust, relevance_table,\n",
    "              K, mincover, sigma_t, credibility=[], bias=[], operations=[],\n",
    "              has_start=True, has_end=False, window_time=None, cluster_list=[], start_nodes=[], end_nodes=[],\n",
    "              verbose=True, force_cluster=True, previous_varsdict=None):\n",
    "    n = len(query.index)  # We can cut out everything after the end.\n",
    "    # Variable names and indices\n",
    "    var_i = []\n",
    "    var_ij = []\n",
    "    var_k = [str(k) for k in range(0, numclust)]\n",
    "\n",
    "    for i in range(0, n):  # This goes up from 0 to n-1.\n",
    "        var_i.append(str(i))\n",
    "        for j in window_i_j[i]:\n",
    "            if i == j:\n",
    "                print(\"ERROR IN WINDOW - BASE\")\n",
    "            var_ij.append(str(i) + \"_\" + str(j))\n",
    "\n",
    "    # Linear program variable declaration.\n",
    "    minedge = LpVariable(\"minedge\", lowBound=0, upBound=1)\n",
    "    node_act_vars = LpVariable.dicts(\"node_act\", var_i, lowBound=0, upBound=1)\n",
    "    node_next_vars = LpVariable.dicts(\"node_next\", var_ij, lowBound=0, upBound=1)\n",
    "    # clust_active_vars = LpVariable.dicts(\"clust_active\", var_k, lowBound=0, upBound=1)\n",
    "\n",
    "    # Create the 'prob' variable to contain the problem data\n",
    "    prob = LpProblem(\"StoryChainProblem\", LpMaximize)\n",
    "    # The objective function is added to 'prob' first\n",
    "    prob += minedge, \"WeakestLink\"\n",
    "\n",
    "    # Chain restrictions\n",
    "    if has_start:\n",
    "        num_starts = len(start_nodes)\n",
    "        if verbose:\n",
    "            print(\"Start node(s):\")\n",
    "            print(start_nodes)\n",
    "        if num_starts == 0:  # This is the default when no list is given and it has a start.\n",
    "            prob += node_act_vars[str(0)] == 1, 'InitialNode'\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Added start node(s)\")\n",
    "                print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "            initial_energy = 1.0 / num_starts\n",
    "            earliest_start = min(start_nodes)\n",
    "            for node in start_nodes:\n",
    "                prob += node_act_vars[str(node)] == initial_energy, 'InitialNode' + str(node)\n",
    "            for node in range(0, earliest_start):\n",
    "                prob += node_act_vars[str(node)] == 0, 'BeforeStart' + str(node)\n",
    "    if has_end:\n",
    "        num_ends = len(end_nodes)\n",
    "        if verbose:\n",
    "            print(\"End node(s):\")\n",
    "            print(end_nodes)\n",
    "        if num_ends == 0:  # This is the default when no list is given and it has a start.\n",
    "            prob += node_act_vars[str(n - 1)] == 1, 'FinalNode'\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Added end node(s)\")\n",
    "                print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "            final_energy = 1.0 / num_ends\n",
    "            latest_end = min(end_nodes)\n",
    "            for node in end_nodes:\n",
    "                prob += node_act_vars[str(node)] == final_energy, 'FinalNode' + str(node)\n",
    "            for node in range(latest_end + 1, n):\n",
    "                prob += node_act_vars[str(node)] == 0, 'AfterEnd' + str(node)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Chain constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    prob += lpSum([node_act_vars[i] for i in var_i]) == K, 'KNodes'\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Expected length constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if has_start:\n",
    "        if verbose:\n",
    "            print(\"Equality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for j in range(1, n):\n",
    "            if j not in start_nodes:\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for i in window_j_i[j]]) == node_act_vars[str(j)], 'InEdgeReq' + str(j)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Generating specific starting node constraints.\")\n",
    "                    print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for i in window_j_i[j]]) == 0, 'InEdgeReq' + str(j)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Inequality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for j in range(1, n):\n",
    "            prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                          for i in window_j_i[j]]) <= node_act_vars[str(j)], 'InEdgeReq' + str(j)\n",
    "    if verbose:\n",
    "        print(\"In-degree constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if has_end:\n",
    "        if verbose:\n",
    "            print(\"Equality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for i in range(0, n - 1):\n",
    "            if i not in end_nodes:\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for j in window_i_j[i]]) == node_act_vars[str(i)], 'OutEdgeReq' + str(i)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Generating specific starting node constraints.\")\n",
    "                    print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for j in window_i_j[i]]) == 0, 'OutEdgeReq' + str(i)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Inequality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for i in range(0, n - 1):\n",
    "            prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                          for j in window_i_j[i]]) <= node_act_vars[str(i)], 'OutEdgeReq' + str(i)\n",
    "    if verbose:\n",
    "        print(\"Out-degree constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # Objective\n",
    "    for i in range(0, n):\n",
    "        for j in window_i_j[i]:\n",
    "            coherence_weights = [0.5, 0.5]\n",
    "            # Five or more entities in common means double the connection strength.\n",
    "            entity_multiplier = min(1 + ent_table[i, j], 2)\n",
    "            # Geometric mean the relevances, multiply based on how far it is from 0.5.\n",
    "            relevance_multiplier = (relevance_table[i] * relevance_table[j]) ** 0.5\n",
    "            coherence = (sim_table[i, j] ** coherence_weights[0]) * \\\n",
    "                (clust_sim_table[i, j] ** coherence_weights[1])\n",
    "            weighted_coherence = min(coherence * entity_multiplier * relevance_multiplier, 1.0)\n",
    "            prob += minedge <= 1 - node_next_vars[str(i) + \"_\" + str(j)] + \\\n",
    "                weighted_coherence, \"Objective\" + str(i) + \"_\" + str(j)\n",
    "    if verbose:\n",
    "        print(\"Objective constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if previous_varsdict:\n",
    "        current_names = [v.name for v in prob.variables() if \"node_act\" in v.name]\n",
    "        if verbose:\n",
    "            print(\"Generated list of names.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for k, v in previous_varsdict.items():\n",
    "            if \"node_act\" in k and k in current_names:\n",
    "                node_act_vars[k.replace(\"node_act_\", \"\")].setInitialValue(v)\n",
    "\n",
    "    if verbose:\n",
    "        if previous_varsdict:\n",
    "            print(\"Used previous solution as starting point.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        else:\n",
    "            print(\"No previous solution available.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    # The problem data is written to an .lp file\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05d7ca",
   "metadata": {},
   "source": [
    "## Building the Graph Data Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920aa6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_df_multiple_starts(query, varsdict, prune=None, threshold=0.01, cluster_dict={}, start_nodes=[]):\n",
    "    n = len(query)\n",
    "    # This has some leftover stuff that is not really useful now.\n",
    "    if 'bias' in query.columns:\n",
    "        graph_df = pd.DataFrame(columns=['id', 'adj_list', 'adj_weights',\n",
    "                                'date', 'publication', 'title', 'text', 'url', 'bias', 'coherence'])\n",
    "    else:\n",
    "        graph_df = pd.DataFrame(columns=['id', 'adj_list', 'adj_weights',\n",
    "                                'date', 'publication', 'title', 'text', 'url', 'coherence'])\n",
    "\n",
    "    already_in = []\n",
    "    for i in range(0, n):\n",
    "        prob = []\n",
    "        coherence = varsdict[\"node_act_\" + str(i)]\n",
    "        if coherence <= threshold:\n",
    "            continue\n",
    "        coherence_list = []\n",
    "        index_list = []\n",
    "        for j in window_i_j[i]:\n",
    "            name = \"node_next_\" + str(i) + \"_\" + str(j)\n",
    "            prob.append(varsdict[name])\n",
    "            coherence_list.append(varsdict[\"node_act_\" + str(j)])\n",
    "        idx_list = [window_i_j[i][idx] for idx, e in enumerate(prob) if round(\n",
    "            e, 8) != 0 and e > threshold and coherence_list[idx] > threshold]  # idx + i + 1\n",
    "        nz_prob = [e for idx, e in enumerate(prob) if round(\n",
    "            e, 8) != 0 and e > threshold and coherence_list[idx] > threshold]\n",
    "        if prune:\n",
    "            if len(idx_list) > prune:\n",
    "                top_prob_idx = sorted(range(len(nz_prob)), key=lambda k: nz_prob[k])[-prune:]\n",
    "                idx_list = [idx_list[j] for j in top_prob_idx]\n",
    "                nz_prob = [nz_prob[idx] for idx in top_prob_idx]\n",
    "        sum_nz = sum(nz_prob)\n",
    "        nz_prob = [nz_prob[j] / sum_nz for j in range(0, len(nz_prob))]\n",
    "        # If we haven't checked this one before we add it to the graph.\n",
    "        url = str(query.iloc[i]['url'])\n",
    "        if i in already_in or sum_nz > 0:\n",
    "            if len(url) > 0:\n",
    "                url = urlparse(url).netloc\n",
    "            if not (graph_df['id'] == i).any():\n",
    "                title = query.iloc[i]['title']\n",
    "                for key, value in cluster_dict.items():\n",
    "                    if str(i) in value:\n",
    "                        title = \"[\" + str(key) + \"] \" + title\n",
    "                outgoing_edges = [idx_temp for idx_temp in idx_list]\n",
    "                # coherence = varsdict[\"node_act_\" + str(i)]\n",
    "                if 'bias' in query.columns:\n",
    "                    graph_df.loc[len(graph_df)] = [i, outgoing_edges, nz_prob, query.iloc[i]['date'], query.iloc[i]['publication'],\n",
    "                                                   title, '', query.iloc[i]['url'], query.iloc[i]['bias'], coherence]\n",
    "                else:\n",
    "                    graph_df.loc[len(graph_df)] = [i, outgoing_edges, nz_prob, query.iloc[i]['date'], query.iloc[i]['publication'],\n",
    "                                                   title, '', query.iloc[i]['url'], coherence]\n",
    "\n",
    "            already_in += [i] + idx_list\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b2a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = None\n",
    "window_i_j = {}\n",
    "window_j_i = {}\n",
    "\n",
    "\n",
    "def solve_LP(\n",
    "    query,\n",
    "    dataset,\n",
    "    membership_vectors,\n",
    "    K=6,\n",
    "    mincover=0.20,\n",
    "    sigma_t=30,\n",
    "    start_nodes=[],\n",
    "    end_nodes=[],\n",
    "    verbose=True,\n",
    "    force_cluster=True,\n",
    "    use_entities=True,\n",
    "    use_temporal=True,\n",
    "    strict_start=False,\n",
    "):\n",
    "\n",
    "    global start_time\n",
    "    start_time = time()\n",
    "\n",
    "    n = len(query.index)\n",
    "    # varsdict_filename = 'varsdict_' + dataset + \"_\" + str(n) + '.pickle'\n",
    "\n",
    "    if sigma_t != 0 and use_temporal:\n",
    "        exp_temp_table = np.exp(-temporal_distance_table / sigma_t)\n",
    "    else:\n",
    "        exp_temp_table = np.ones(temporal_distance_table.shape)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed temporal distance table.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    window_time = None\n",
    "    if sigma_t != 0 and use_temporal:\n",
    "        window_time = sigma_t * 3  # Days\n",
    "\n",
    "    if window_time is None:\n",
    "        for i in range(0, n):\n",
    "            window_i_j[i] = list(range(i + 1, n))\n",
    "        for j in range(0, n):\n",
    "            window_j_i[j] = list(range(0, j))\n",
    "    else:\n",
    "        for j in range(0, n):\n",
    "            window_j_i[j] = []\n",
    "        for i in range(0, n):\n",
    "            window_i_j[i] = []\n",
    "        for i in range(0, n - 1):\n",
    "            window = 0\n",
    "            for j in range(i + 1, n):\n",
    "                if temporal_distance_table[i, j] <= window_time:\n",
    "                    window += 1\n",
    "            window = max(min(5, n - i), window)\n",
    "            window_i_j[i] = list(range(i + 1, min(i + window, n)))\n",
    "            for j in window_i_j[i]:\n",
    "                window_j_i[j].append(i)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed temporal windows.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed entity similarities.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    ent_table = np.zeros((n, n))  # Fill entity information with zeros by default.\n",
    "    actual_ent_table = ent_table\n",
    "    ent_doc_list = None\n",
    "    if use_entities:\n",
    "        ent_table, ent_doc_list = get_entity_table(query, dataset)\n",
    "        actual_ent_table = ent_table\n",
    "\n",
    "    # Deprecated relevance table computation\n",
    "    relevance_table = [1.0] * membership_vectors.shape[0]  # Create a vector full of 1s\n",
    "\n",
    "    has_start = False\n",
    "    if start_nodes is not None:\n",
    "        has_start = (len(start_nodes) > 0)\n",
    "    if end_nodes is not None:\n",
    "        has_end = (len(end_nodes) > 0)\n",
    "    if verbose:\n",
    "        print(\"Creating LP...\")\n",
    "\n",
    "    # Read previous solution and feed to LP. If none there is no previous solution.\n",
    "    previous_varsdict = None\n",
    "    # if os.path.isfile(varsdict_filename):\n",
    "    #     with open(varsdict_filename, 'rb') as handle:\n",
    "    #         previous_varsdict = pickle.load(handle)\n",
    "\n",
    "    prob = create_LP(\n",
    "        query,\n",
    "        sim_table,\n",
    "        membership_vectors,\n",
    "        clust_sim_table,\n",
    "        exp_temp_table,\n",
    "        actual_ent_table,\n",
    "        numclust,\n",
    "        relevance_table,\n",
    "        K=K,\n",
    "        mincover=mincover,\n",
    "        sigma_t=sigma_t,\n",
    "        has_start=has_start,\n",
    "        has_end=has_end,\n",
    "        start_nodes=start_nodes,\n",
    "        end_nodes=end_nodes,\n",
    "        verbose=verbose,\n",
    "        force_cluster=force_cluster,\n",
    "        previous_varsdict=previous_varsdict\n",
    "    )\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(\"Saving model...\")\n",
    "    #     print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # prob.writeLP(\"left_story.lp\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Solving model...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # (GLPK_CMD(path = 'C:\\\\glpk-4.65\\\\w64\\\\glpsol.exe', options = [\"--tmlim\", \"180\"]))\n",
    "\n",
    "    prob.solve(PULP_CBC_CMD(mip=False, warmStart=True, msg=verbose))\n",
    "\n",
    "    varsdict = extract_varsdict(prob)\n",
    "\n",
    "    # Overwrite last solution.\n",
    "    # with open(varsdict_filename, 'wb') as handle:\n",
    "    #     pickle.dump(varsdict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    graph_df = build_graph_df_multiple_starts(query, varsdict, prune=ceil(\n",
    "        sqrt(K)), threshold=0.1 / K, cluster_dict={})\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Graph data frame construction...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if strict_start and has_start:\n",
    "        graph_df = graph_clean_up(graph_df, start_nodes)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Graph clean up...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    return [graph_df, (numclust, LpStatus[prob.status]), sim_table, clust_sim_table, ent_table, ent_doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378d128",
   "metadata": {},
   "source": [
    "## MAIN: Calling the Map Construction Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c530c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../../data/VisPubData/embed_data-gpt4.pickle' loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>Conference</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>DOI</th>\n",
       "      <th>url</th>\n",
       "      <th>FirstPage</th>\n",
       "      <th>LastPage</th>\n",
       "      <th>PaperType</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>AuthorNames</th>\n",
       "      <th>AuthorAffiliation</th>\n",
       "      <th>InternalReferences</th>\n",
       "      <th>AuthorKeywords</th>\n",
       "      <th>AminerCitationCount</th>\n",
       "      <th>CitationCount_CrossRef</th>\n",
       "      <th>PubsCited_CrossRef</th>\n",
       "      <th>Award</th>\n",
       "      <th>embed</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3513</td>\n",
       "      <td>Vis</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>Volume visualization in cell biology</td>\n",
       "      <td>10.1109/VISUAL.1990.146378</td>\n",
       "      <td>http://dx.doi.org/10.1109/VISUAL.1990.146378</td>\n",
       "      <td>160</td>\n",
       "      <td>168, 471-2</td>\n",
       "      <td>C</td>\n",
       "      <td>The authors discuss the special properties of ...</td>\n",
       "      <td>...</td>\n",
       "      <td>A. Kaufman;R. Yagel;R. Bakalash;I. Spector</td>\n",
       "      <td>Department of Computer Sciences, State Univers...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.029562540352344513, 0.005896927323192358, 0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3498</td>\n",
       "      <td>Vis</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>Exploring N-dimensional databases</td>\n",
       "      <td>10.1109/VISUAL.1990.146386</td>\n",
       "      <td>http://dx.doi.org/10.1109/VISUAL.1990.146386</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>C</td>\n",
       "      <td>The authors present a tool for the display and...</td>\n",
       "      <td>...</td>\n",
       "      <td>J. LeBlanc;M.O. Ward;N. Wittels</td>\n",
       "      <td>Computer Science Department, Worcester Polytec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>415.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.03208579868078232, 0.010630691424012184, 0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3515</td>\n",
       "      <td>Vis</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>Interactive visualization of quaternion Julia ...</td>\n",
       "      <td>10.1109/VISUAL.1990.146384</td>\n",
       "      <td>http://dx.doi.org/10.1109/VISUAL.1990.146384</td>\n",
       "      <td>209</td>\n",
       "      <td>218, 475-6</td>\n",
       "      <td>C</td>\n",
       "      <td>The first half of a two-step quaternion Julia ...</td>\n",
       "      <td>...</td>\n",
       "      <td>J.C. Hart;L.H. Kauffman;D.J. Sandim</td>\n",
       "      <td>Electronic Visualization Laboratory, Universit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.046963874250650406, -0.004976061638444662,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3526</td>\n",
       "      <td>Vis</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>Accurate display of tensor product isosurfaces</td>\n",
       "      <td>10.1109/VISUAL.1990.146401</td>\n",
       "      <td>http://dx.doi.org/10.1109/VISUAL.1990.146401</td>\n",
       "      <td>353</td>\n",
       "      <td>360, 489</td>\n",
       "      <td>C</td>\n",
       "      <td>A general method for rendering isosurfaces of ...</td>\n",
       "      <td>...</td>\n",
       "      <td>A. Rockwood</td>\n",
       "      <td>Silicon Graphics Computer Systems, Mountain Vi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.04734494909644127, -0.04052385315299034, -...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3548</td>\n",
       "      <td>Vis</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>The application of transport theory to visuali...</td>\n",
       "      <td>10.1109/VISUAL.1990.146391</td>\n",
       "      <td>http://dx.doi.org/10.1109/VISUAL.1990.146391</td>\n",
       "      <td>273</td>\n",
       "      <td>280, 481-2</td>\n",
       "      <td>C</td>\n",
       "      <td>The author describes a visualization model for...</td>\n",
       "      <td>...</td>\n",
       "      <td>W. Krueger</td>\n",
       "      <td>ART COM e.V., Berlin, Germany</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.037166863679885864, 0.026174204424023628, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>26</td>\n",
       "      <td>Vis</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PuzzleFixer: A Visual Reassembly System for Im...</td>\n",
       "      <td>10.1109/TVCG.2022.3209388</td>\n",
       "      <td>http://dx.doi.org/10.1109/TVCG.2022.3209388</td>\n",
       "      <td>429</td>\n",
       "      <td>439</td>\n",
       "      <td>J</td>\n",
       "      <td>We present PuzzleFixer, an immersive interacti...</td>\n",
       "      <td>...</td>\n",
       "      <td>Shuainan Ye;Zhutian Chen;Xiangtong Chu;Kang Li...</td>\n",
       "      <td>State Key Lab of CAD&amp;CG, Zhejiang University, ...</td>\n",
       "      <td>10.1109/TVCG.2019.2934332;10.1109/TVCG.2021.31...</td>\n",
       "      <td>Immersive visualization,interactive exploratio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.018821651116013527, 0.05129850283265114, 0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>109</td>\n",
       "      <td>Vis</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>A Design Space for Surfacing Content Recommend...</td>\n",
       "      <td>10.1109/TVCG.2022.3209445</td>\n",
       "      <td>http://dx.doi.org/10.1109/TVCG.2022.3209445</td>\n",
       "      <td>84</td>\n",
       "      <td>94</td>\n",
       "      <td>J</td>\n",
       "      <td>Recommendation algorithms have been leveraged ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Zhilan Zhou;Wenyuan Wang;Mengtian Guo;Yue Wang...</td>\n",
       "      <td>Department of Computer Science, University of ...</td>\n",
       "      <td>10.1109/VAST.2008.4677373;10.1109/TVCG.2017.27...</td>\n",
       "      <td>Adaptive Visualization,Recommendation,Literatu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.01194240152835846, 0.01570834591984749, 0.0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>64</td>\n",
       "      <td>Vis</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Predicting User Preferences of Dimensionality ...</td>\n",
       "      <td>10.1109/TVCG.2022.3209449</td>\n",
       "      <td>http://dx.doi.org/10.1109/TVCG.2022.3209449</td>\n",
       "      <td>745</td>\n",
       "      <td>755</td>\n",
       "      <td>J</td>\n",
       "      <td>A plethora of dimensionality reduction techniq...</td>\n",
       "      <td>...</td>\n",
       "      <td>Cristina Morariu;Adrien Bibal;Rene Cutura;Beno...</td>\n",
       "      <td>University of Stuttgart, Germany;Université ca...</td>\n",
       "      <td>10.1109/TVCG.2011.229;10.1109/VAST.2010.565239...</td>\n",
       "      <td>Dimensionality reduction,Manifold learning,Hum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.011614157818257809, -0.023910094052553177, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>59</td>\n",
       "      <td>Vis</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>RankAxis: Towards a Systematic Combination of ...</td>\n",
       "      <td>10.1109/TVCG.2022.3209463</td>\n",
       "      <td>http://dx.doi.org/10.1109/TVCG.2022.3209463</td>\n",
       "      <td>701</td>\n",
       "      <td>711</td>\n",
       "      <td>J</td>\n",
       "      <td>Projection and ranking are frequently used ana...</td>\n",
       "      <td>...</td>\n",
       "      <td>Qiangqiang Liu;Yukun Ren;Zhihua Zhu;Dai Li;Xia...</td>\n",
       "      <td>ShanghaiTech, China;Corporate Development Grou...</td>\n",
       "      <td>10.1109/VAST.2010.5652433;10.1109/VAST.2018.88...</td>\n",
       "      <td>Ranking,projection,multi-attribute data explor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.01526233647018671, 0.02215460315346718, 0....</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>58</td>\n",
       "      <td>Vis</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Visualizing the Passage of Time with Video Tem...</td>\n",
       "      <td>10.1109/TVCG.2022.3209454</td>\n",
       "      <td>http://dx.doi.org/10.1109/TVCG.2022.3209454</td>\n",
       "      <td>171</td>\n",
       "      <td>181</td>\n",
       "      <td>J</td>\n",
       "      <td>What can we learn about a scene by watching it...</td>\n",
       "      <td>...</td>\n",
       "      <td>Melissa E. Swift;Wyatt Ayers;Sophie Pallanck;S...</td>\n",
       "      <td>Western Washington University, USA;Western Was...</td>\n",
       "      <td>10.1109/TVCG.2020.3030398;10.1109/TVCG.2012.22...</td>\n",
       "      <td>Time,time-frequency,video visualization,multi-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.02051727846264839, 0.01476989220827818, -0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx Conference       date  \\\n",
       "0    3513        Vis 1990-01-01   \n",
       "1    3498        Vis 1990-01-01   \n",
       "2    3515        Vis 1990-01-01   \n",
       "3    3526        Vis 1990-01-01   \n",
       "4    3548        Vis 1990-01-01   \n",
       "..    ...        ...        ...   \n",
       "835    26        Vis 2022-01-01   \n",
       "836   109        Vis 2022-01-01   \n",
       "837    64        Vis 2022-01-01   \n",
       "838    59        Vis 2022-01-01   \n",
       "839    58        Vis 2022-01-01   \n",
       "\n",
       "                                                 title  \\\n",
       "0                 Volume visualization in cell biology   \n",
       "1                    Exploring N-dimensional databases   \n",
       "2    Interactive visualization of quaternion Julia ...   \n",
       "3       Accurate display of tensor product isosurfaces   \n",
       "4    The application of transport theory to visuali...   \n",
       "..                                                 ...   \n",
       "835  PuzzleFixer: A Visual Reassembly System for Im...   \n",
       "836  A Design Space for Surfacing Content Recommend...   \n",
       "837  Predicting User Preferences of Dimensionality ...   \n",
       "838  RankAxis: Towards a Systematic Combination of ...   \n",
       "839  Visualizing the Passage of Time with Video Tem...   \n",
       "\n",
       "                            DOI                                           url  \\\n",
       "0    10.1109/VISUAL.1990.146378  http://dx.doi.org/10.1109/VISUAL.1990.146378   \n",
       "1    10.1109/VISUAL.1990.146386  http://dx.doi.org/10.1109/VISUAL.1990.146386   \n",
       "2    10.1109/VISUAL.1990.146384  http://dx.doi.org/10.1109/VISUAL.1990.146384   \n",
       "3    10.1109/VISUAL.1990.146401  http://dx.doi.org/10.1109/VISUAL.1990.146401   \n",
       "4    10.1109/VISUAL.1990.146391  http://dx.doi.org/10.1109/VISUAL.1990.146391   \n",
       "..                          ...                                           ...   \n",
       "835   10.1109/TVCG.2022.3209388   http://dx.doi.org/10.1109/TVCG.2022.3209388   \n",
       "836   10.1109/TVCG.2022.3209445   http://dx.doi.org/10.1109/TVCG.2022.3209445   \n",
       "837   10.1109/TVCG.2022.3209449   http://dx.doi.org/10.1109/TVCG.2022.3209449   \n",
       "838   10.1109/TVCG.2022.3209463   http://dx.doi.org/10.1109/TVCG.2022.3209463   \n",
       "839   10.1109/TVCG.2022.3209454   http://dx.doi.org/10.1109/TVCG.2022.3209454   \n",
       "\n",
       "    FirstPage    LastPage PaperType  \\\n",
       "0         160  168, 471-2         C   \n",
       "1         230         237         C   \n",
       "2         209  218, 475-6         C   \n",
       "3         353    360, 489         C   \n",
       "4         273  280, 481-2         C   \n",
       "..        ...         ...       ...   \n",
       "835       429         439         J   \n",
       "836        84          94         J   \n",
       "837       745         755         J   \n",
       "838       701         711         J   \n",
       "839       171         181         J   \n",
       "\n",
       "                                              Abstract  ...  \\\n",
       "0    The authors discuss the special properties of ...  ...   \n",
       "1    The authors present a tool for the display and...  ...   \n",
       "2    The first half of a two-step quaternion Julia ...  ...   \n",
       "3    A general method for rendering isosurfaces of ...  ...   \n",
       "4    The author describes a visualization model for...  ...   \n",
       "..                                                 ...  ...   \n",
       "835  We present PuzzleFixer, an immersive interacti...  ...   \n",
       "836  Recommendation algorithms have been leveraged ...  ...   \n",
       "837  A plethora of dimensionality reduction techniq...  ...   \n",
       "838  Projection and ranking are frequently used ana...  ...   \n",
       "839  What can we learn about a scene by watching it...  ...   \n",
       "\n",
       "                                           AuthorNames  \\\n",
       "0           A. Kaufman;R. Yagel;R. Bakalash;I. Spector   \n",
       "1                      J. LeBlanc;M.O. Ward;N. Wittels   \n",
       "2                  J.C. Hart;L.H. Kauffman;D.J. Sandim   \n",
       "3                                          A. Rockwood   \n",
       "4                                           W. Krueger   \n",
       "..                                                 ...   \n",
       "835  Shuainan Ye;Zhutian Chen;Xiangtong Chu;Kang Li...   \n",
       "836  Zhilan Zhou;Wenyuan Wang;Mengtian Guo;Yue Wang...   \n",
       "837  Cristina Morariu;Adrien Bibal;Rene Cutura;Beno...   \n",
       "838  Qiangqiang Liu;Yukun Ren;Zhihua Zhu;Dai Li;Xia...   \n",
       "839  Melissa E. Swift;Wyatt Ayers;Sophie Pallanck;S...   \n",
       "\n",
       "                                     AuthorAffiliation  \\\n",
       "0    Department of Computer Sciences, State Univers...   \n",
       "1    Computer Science Department, Worcester Polytec...   \n",
       "2    Electronic Visualization Laboratory, Universit...   \n",
       "3    Silicon Graphics Computer Systems, Mountain Vi...   \n",
       "4                        ART COM e.V., Berlin, Germany   \n",
       "..                                                 ...   \n",
       "835  State Key Lab of CAD&CG, Zhejiang University, ...   \n",
       "836  Department of Computer Science, University of ...   \n",
       "837  University of Stuttgart, Germany;Université ca...   \n",
       "838  ShanghaiTech, China;Corporate Development Grou...   \n",
       "839  Western Washington University, USA;Western Was...   \n",
       "\n",
       "                                    InternalReferences  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "835  10.1109/TVCG.2019.2934332;10.1109/TVCG.2021.31...   \n",
       "836  10.1109/VAST.2008.4677373;10.1109/TVCG.2017.27...   \n",
       "837  10.1109/TVCG.2011.229;10.1109/VAST.2010.565239...   \n",
       "838  10.1109/VAST.2010.5652433;10.1109/VAST.2018.88...   \n",
       "839  10.1109/TVCG.2020.3030398;10.1109/TVCG.2012.22...   \n",
       "\n",
       "                                        AuthorKeywords AminerCitationCount  \\\n",
       "0                                                  NaN                37.0   \n",
       "1                                                  NaN               415.0   \n",
       "2                                                  NaN                32.0   \n",
       "3                                                  NaN                12.0   \n",
       "4                                                  NaN                 NaN   \n",
       "..                                                 ...                 ...   \n",
       "835  Immersive visualization,interactive exploratio...                 NaN   \n",
       "836  Adaptive Visualization,Recommendation,Literatu...                 NaN   \n",
       "837  Dimensionality reduction,Manifold learning,Hum...                 NaN   \n",
       "838  Ranking,projection,multi-attribute data explor...                 NaN   \n",
       "839  Time,time-frequency,video visualization,multi-...                 NaN   \n",
       "\n",
       "     CitationCount_CrossRef  PubsCited_CrossRef  Award  \\\n",
       "0                      10.0                29.0    NaN   \n",
       "1                      99.0                16.0    NaN   \n",
       "2                       5.0                11.0    NaN   \n",
       "3                       5.0                19.0    NaN   \n",
       "4                      10.0                24.0    NaN   \n",
       "..                      ...                 ...    ...   \n",
       "835                     0.0                73.0    NaN   \n",
       "836                     0.0                83.0    NaN   \n",
       "837                     1.0                54.0    NaN   \n",
       "838                     0.0                52.0    NaN   \n",
       "839                     0.0                51.0    NaN   \n",
       "\n",
       "                                                 embed publication  \n",
       "0    [0.029562540352344513, 0.005896927323192358, 0...              \n",
       "1    [-0.03208579868078232, 0.010630691424012184, 0...              \n",
       "2    [-0.046963874250650406, -0.004976061638444662,...              \n",
       "3    [-0.04734494909644127, -0.04052385315299034, -...              \n",
       "4    [-0.037166863679885864, 0.026174204424023628, ...              \n",
       "..                                                 ...         ...  \n",
       "835  [-0.018821651116013527, 0.05129850283265114, 0...              \n",
       "836  [0.01194240152835846, 0.01570834591984749, 0.0...              \n",
       "837  [0.011614157818257809, -0.023910094052553177, ...              \n",
       "838  [-0.01526233647018671, 0.02215460315346718, 0....              \n",
       "839  [-0.02051727846264839, 0.01476989220827818, -0...              \n",
       "\n",
       "[840 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vis_pub = pd.read_csv(f\"../../data/VisPubData/text_data.csv\")\n",
    "data_vis_pub[\"Year\"] = pd.to_datetime(data_vis_pub[\"Year\"], format=\"%Y\")\n",
    "data_vis_pub.rename(columns={\"Year\": \"date\"}, inplace=True)\n",
    "\n",
    "# Drop rows with no abstract in VisPubData\n",
    "data_vis_pub = data_vis_pub[~(data_vis_pub[\"Abstract\"].isna())].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Load embeddings for VisPubData\n",
    "vis_pub_embeds, _, _ = extract_embeddings(\n",
    "    text=(data_vis_pub[\"Title\"] + \";\" + data_vis_pub[\"Abstract\"]).tolist(),\n",
    "    foldername=f\"../../data/VisPubData\",\n",
    "    model_name=\"gpt4\"\n",
    ")\n",
    "\n",
    "data_vis_pub[\"embed\"] = vis_pub_embeds.tolist()\n",
    "data_vis_pub[\"publication\"] = \"\"\n",
    "data_vis_pub.rename(columns={\"Title\": \"title\", \"Link\": \"url\"}, inplace=True)\n",
    "data_vis_pub.sort_values(by=\"date\").reset_index(names=\"idx\")\n",
    "\n",
    "\n",
    "data_vis_pub = data_vis_pub.sample(840, replace=False, random_state=420).sort_values(by=\"date\")\n",
    "data_vis_pub.reset_index(names=\"idx\", inplace=True)\n",
    "\n",
    "\n",
    "# select sources and targets\n",
    "vis_pub_src = np.random.choice(data_vis_pub.index[:(len(data_vis_pub) // 2) - 50], 50)\n",
    "vis_pub_tgt = np.random.choice(data_vis_pub.index[(len(data_vis_pub) // 2) + 50:], 50)\n",
    "\n",
    "data_vis_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5193cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 Topics\n"
     ]
    }
   ],
   "source": [
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=32,\n",
    "    n_components=48,\n",
    "    min_dist=0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    n_jobs=1,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "low_dim_mapper = umap_model.fit(np.array(data_vis_pub[\"embed\"].tolist()))\n",
    "low_dim_embeds = low_dim_mapper.embedding_\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ").fit(low_dim_embeds)\n",
    "\n",
    "cluster_label_probs = hdbscan.prediction.all_points_membership_vectors(\n",
    "    hdbscan_model\n",
    ")\n",
    "\n",
    "cluster_labels = cluster_label_probs.argmax(1)\n",
    "\n",
    "data_vis_pub[\"topic\"] = cluster_labels\n",
    "print(f\"Found {len(np.unique(cluster_labels))} Topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29150ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Length (usually values from 6 to 12 produce decent maps, but it depends on data set size and probably the underlying distribution of similarities).\n",
    "k_input = 12\n",
    "\n",
    "# % of average coverage we require. For small data sets 50-80 is good. For bigger data sets with many clusters you will likely only get 20%.\n",
    "# This was tested with values up to 500. After that I'm not sure how well the model will perform.\n",
    "mincover_input = 0\n",
    "\n",
    "# Temporal distance penalty in DAYS. I left it on 30 as default for the Cuban data set.\n",
    "# Lower values allow more temporally distant connections. Consider temporal density of the data when adjusting.\n",
    "# Can set it to 0 and it will be discarded from the computation.\n",
    "sigma_t = 0\n",
    "use_temporal = False  # Use this to enable or disable the temporal penalty, by default it is on.\n",
    "\n",
    "# Leave this as false, there was supposed to be a reward factor for events with common entities, but it adds too much computational time so not worth it.\n",
    "use_entities = False\n",
    "\n",
    "# If you enable strict start you will discard any storyline that does not start from the user-defined start node.\n",
    "# It is recommended to disable this to allow for extra storylines that emerge from the LP solution.\n",
    "strict_start = False\n",
    "\n",
    "# Compute angular similarity\n",
    "similarities = np.clip(cosine_similarity(np.array(data_vis_pub[\"embed\"].tolist())), -1, 1)\n",
    "sim_table = (1 - np.arccos(similarities) / pi)\n",
    "mask = np.ones(sim_table.shape, dtype=bool)\n",
    "np.fill_diagonal(mask, 0)\n",
    "max_value = sim_table[mask].max()\n",
    "min_value = sim_table[mask].min()\n",
    "sim_table = (sim_table - min_value) / (max_value - min_value)\n",
    "sim_table = np.clip(sim_table, 0, 1)\n",
    "\n",
    "# Compute topic similarity\n",
    "numclust = 1\n",
    "clust_sim = np.zeros((cluster_label_probs.shape[0], cluster_label_probs.shape[0]))\n",
    "\n",
    "if len(cluster_label_probs.shape) > 1:\n",
    "    numclust = cluster_label_probs.shape[1]\n",
    "    cluster_label_probs[cluster_label_probs < 1 / numclust] = 0\n",
    "    cluster_label_probs[np.all(cluster_label_probs == 0,\n",
    "                               axis=1)] = np.ones(numclust) / numclust\n",
    "    row_sums = cluster_label_probs.sum(axis=1)\n",
    "    cluster_label_probs = cluster_label_probs / row_sums[:, np.newaxis]\n",
    "\n",
    "    clust_sim = distance.cdist(\n",
    "        cluster_label_probs,\n",
    "        cluster_label_probs,\n",
    "        lambda u, v: distance.jensenshannon(u, v, base=2.0)\n",
    "    )\n",
    "else:\n",
    "    cluster_label_probs = np.ones((cluster_label_probs.shape[0], 1))\n",
    "\n",
    "clust_sim_table = 1 - clust_sim\n",
    "\n",
    "# Compute temporal distance\n",
    "temporal_distance_table = compute_temp_distance_table(data_vis_pub, \"./narrative_maps/temp/vispub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78300ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:22:06<00:00, 98.54s/it] \n"
     ]
    }
   ],
   "source": [
    "results_data = {\n",
    "    \"algorithm\": [],\n",
    "    \"src\": [],\n",
    "    \"tgt\": [],\n",
    "    \"exec_time\": [],\n",
    "    \"effective_exec_time\": [],\n",
    "    \"main_storyline\": [],\n",
    "    \"storylines\": []\n",
    "}\n",
    "\n",
    "for src, tgt in tqdm(zip(vis_pub_src, vis_pub_tgt), total=len(vis_pub_src)):\n",
    "    for i in range(4):\n",
    "        start_time = time()\n",
    "        graph_df_new, status, _, _, _, _ = solve_LP(\n",
    "            data_vis_pub,\n",
    "            dataset=\"vispub\",\n",
    "            membership_vectors=cluster_label_probs,\n",
    "            K=k_input,\n",
    "            mincover=mincover_input / 100,\n",
    "            sigma_t=sigma_t,\n",
    "            start_nodes=[src],\n",
    "            end_nodes=[tgt],\n",
    "            verbose=False,\n",
    "            use_entities=use_entities,\n",
    "            use_temporal=use_temporal,\n",
    "            strict_start=strict_start,\n",
    "        )\n",
    "        end_time = time() - start_time\n",
    "\n",
    "        # Post Processing\n",
    "        if 'Optimal' in status[1]:\n",
    "            G = build_graph(graph_df_new)\n",
    "            storylines = graph_stories(G, start_nodes=[src], end_nodes=[tgt])\n",
    "\n",
    "            results_data[\"algorithm\"].append(\"narrative_maps\")\n",
    "            results_data[\"src\"].append(src)\n",
    "            results_data[\"tgt\"].append(tgt)\n",
    "            results_data[\"exec_time\"].append(end_time)\n",
    "            results_data[\"effective_exec_time\"].append(end_time / len(storylines))\n",
    "            results_data[\"main_storyline\"].append(storylines[0])\n",
    "            results_data[\"storylines\"].append(storylines)\n",
    "        else:\n",
    "            print(f\"** Warning: Experiment '({src}, {tgt})' not optimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644f252f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>exec_time</th>\n",
       "      <th>effective_exec_time</th>\n",
       "      <th>main_storyline</th>\n",
       "      <th>storylines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>268</td>\n",
       "      <td>827</td>\n",
       "      <td>24.428430</td>\n",
       "      <td>4.885686</td>\n",
       "      <td>[268, 270, 393, 417, 469, 516, 556, 621, 641, ...</td>\n",
       "      <td>[[268, 270, 393, 417, 469, 516, 556, 621, 641,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>20</td>\n",
       "      <td>740</td>\n",
       "      <td>22.631616</td>\n",
       "      <td>4.526323</td>\n",
       "      <td>[20, 122, 317, 421, 490, 638, 740]</td>\n",
       "      <td>[[20, 122, 317, 421, 490, 638, 740], [72, 183,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>118</td>\n",
       "      <td>675</td>\n",
       "      <td>25.433000</td>\n",
       "      <td>2.543300</td>\n",
       "      <td>[118, 310, 627, 675]</td>\n",
       "      <td>[[118, 310, 627, 675], [170, 243, 286, 345], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>40</td>\n",
       "      <td>830</td>\n",
       "      <td>20.968464</td>\n",
       "      <td>2.995495</td>\n",
       "      <td>[40, 452, 466, 611, 632, 657, 690, 795, 830]</td>\n",
       "      <td>[[40, 452, 466, 611, 632, 657, 690, 795, 830],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>98</td>\n",
       "      <td>480</td>\n",
       "      <td>25.840124</td>\n",
       "      <td>3.691446</td>\n",
       "      <td>[98, 155, 224, 247, 272, 327, 346, 412, 426, 4...</td>\n",
       "      <td>[[98, 155, 224, 247, 272, 327, 346, 412, 426, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>104</td>\n",
       "      <td>477</td>\n",
       "      <td>26.296817</td>\n",
       "      <td>2.921869</td>\n",
       "      <td>[104, 106, 197, 242, 247, 331, 410, 412, 452, ...</td>\n",
       "      <td>[[104, 106, 197, 242, 247, 331, 410, 412, 452,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>29</td>\n",
       "      <td>698</td>\n",
       "      <td>23.545890</td>\n",
       "      <td>2.616210</td>\n",
       "      <td>[29, 183, 309, 320, 375, 456, 608, 649, 698]</td>\n",
       "      <td>[[29, 183, 309, 320, 375, 456, 608, 649, 698],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>246</td>\n",
       "      <td>763</td>\n",
       "      <td>24.837642</td>\n",
       "      <td>4.967528</td>\n",
       "      <td>[246, 341, 357, 532, 546, 610, 621, 699, 755, ...</td>\n",
       "      <td>[[246, 341, 357, 532, 546, 610, 621, 699, 755,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>20</td>\n",
       "      <td>740</td>\n",
       "      <td>22.823429</td>\n",
       "      <td>4.564686</td>\n",
       "      <td>[20, 122, 317, 421, 490, 638, 740]</td>\n",
       "      <td>[[20, 122, 317, 421, 490, 638, 740], [72, 183,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>231</td>\n",
       "      <td>760</td>\n",
       "      <td>22.892366</td>\n",
       "      <td>4.578473</td>\n",
       "      <td>[231, 430, 431, 451, 470, 511, 515, 575, 661, ...</td>\n",
       "      <td>[[231, 430, 431, 451, 470, 511, 515, 575, 661,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          algorithm  src  tgt  exec_time  effective_exec_time  \\\n",
       "62   narrative_maps  268  827  24.428430             4.885686   \n",
       "157  narrative_maps   20  740  22.631616             4.526323   \n",
       "147  narrative_maps  118  675  25.433000             2.543300   \n",
       "168  narrative_maps   40  830  20.968464             2.995495   \n",
       "8    narrative_maps   98  480  25.840124             3.691446   \n",
       "39   narrative_maps  104  477  26.296817             2.921869   \n",
       "176  narrative_maps   29  698  23.545890             2.616210   \n",
       "18   narrative_maps  246  763  24.837642             4.967528   \n",
       "159  narrative_maps   20  740  22.823429             4.564686   \n",
       "191  narrative_maps  231  760  22.892366             4.578473   \n",
       "\n",
       "                                        main_storyline  \\\n",
       "62   [268, 270, 393, 417, 469, 516, 556, 621, 641, ...   \n",
       "157                 [20, 122, 317, 421, 490, 638, 740]   \n",
       "147                               [118, 310, 627, 675]   \n",
       "168       [40, 452, 466, 611, 632, 657, 690, 795, 830]   \n",
       "8    [98, 155, 224, 247, 272, 327, 346, 412, 426, 4...   \n",
       "39   [104, 106, 197, 242, 247, 331, 410, 412, 452, ...   \n",
       "176       [29, 183, 309, 320, 375, 456, 608, 649, 698]   \n",
       "18   [246, 341, 357, 532, 546, 610, 621, 699, 755, ...   \n",
       "159                 [20, 122, 317, 421, 490, 638, 740]   \n",
       "191  [231, 430, 431, 451, 470, 511, 515, 575, 661, ...   \n",
       "\n",
       "                                            storylines  \n",
       "62   [[268, 270, 393, 417, 469, 516, 556, 621, 641,...  \n",
       "157  [[20, 122, 317, 421, 490, 638, 740], [72, 183,...  \n",
       "147  [[118, 310, 627, 675], [170, 243, 286, 345], [...  \n",
       "168  [[40, 452, 466, 611, 632, 657, 690, 795, 830],...  \n",
       "8    [[98, 155, 224, 247, 272, 327, 346, 412, 426, ...  \n",
       "39   [[104, 106, 197, 242, 247, 331, 410, 412, 452,...  \n",
       "176  [[29, 183, 309, 320, 375, 456, 608, 649, 698],...  \n",
       "18   [[246, 341, 357, 532, 546, 610, 621, 699, 755,...  \n",
       "159  [[20, 122, 317, 421, 490, 638, 740], [72, 183,...  \n",
       "191  [[231, 430, 431, 451, 470, 511, 515, 575, 661,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_data_df = pd.DataFrame(results_data)\n",
    "results_data_df.sample(10)  # Show 10 example results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8273d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data_df.to_pickle(\"./narrative_maps/results/vispub.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9add7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrative-trails",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
