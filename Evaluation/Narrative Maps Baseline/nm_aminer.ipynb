{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ae241f",
   "metadata": {},
   "source": [
    "# Narrative Maps Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6fa888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# To make our relative library imports work\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f695cc",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b431581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustogerman/miniforge3/envs/narrative-trails/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "import math\n",
    "from math import log, exp, pi, sqrt, ceil\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "from pulp import *\n",
    "\n",
    "from Library.embedding_extraction import extract_embeddings\n",
    "\n",
    "from narrative_maps import (\n",
    "    extract_varsdict,\n",
    "    compute_temp_distance_table,\n",
    "    build_graph,\n",
    "    graph_stories,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67bcfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fddf4",
   "metadata": {},
   "source": [
    "## Linear Program Construction\n",
    "\n",
    "This has a lot of parameters, some of them ended up unused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa154a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LP(query, sim_table, membership_vectors, clust_sim_table, exp_temp_table, ent_table, numclust, relevance_table,\n",
    "              K, mincover, sigma_t, credibility=[], bias=[], operations=[],\n",
    "              has_start=True, has_end=False, window_time=None, cluster_list=[], start_nodes=[], end_nodes=[],\n",
    "              verbose=True, force_cluster=True, previous_varsdict=None):\n",
    "    n = len(query.index)  # We can cut out everything after the end.\n",
    "    # Variable names and indices\n",
    "    var_i = []\n",
    "    var_ij = []\n",
    "    var_k = [str(k) for k in range(0, numclust)]\n",
    "\n",
    "    for i in range(0, n):  # This goes up from 0 to n-1.\n",
    "        var_i.append(str(i))\n",
    "        for j in window_i_j[i]:\n",
    "            if i == j:\n",
    "                print(\"ERROR IN WINDOW - BASE\")\n",
    "            var_ij.append(str(i) + \"_\" + str(j))\n",
    "\n",
    "    # Linear program variable declaration.\n",
    "    minedge = LpVariable(\"minedge\", lowBound=0, upBound=1)\n",
    "    node_act_vars = LpVariable.dicts(\"node_act\", var_i, lowBound=0, upBound=1)\n",
    "    node_next_vars = LpVariable.dicts(\"node_next\", var_ij, lowBound=0,  upBound=1)\n",
    "    # clust_active_vars = LpVariable.dicts(\"clust_active\", var_k, lowBound=0, upBound=1)\n",
    "\n",
    "    # Create the 'prob' variable to contain the problem data\n",
    "    prob = LpProblem(\"StoryChainProblem\", LpMaximize)\n",
    "    # The objective function is added to 'prob' first\n",
    "    prob += minedge, \"WeakestLink\"\n",
    "\n",
    "    # Chain restrictions\n",
    "    if has_start:\n",
    "        num_starts = len(start_nodes)\n",
    "        if verbose:\n",
    "            print(\"Start node(s):\")\n",
    "            print(start_nodes)\n",
    "        if num_starts == 0:  # This is the default when no list is given and it has a start.\n",
    "            prob += node_act_vars[str(0)] == 1, 'InitialNode'\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Added start node(s)\")\n",
    "                print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "            initial_energy = 1.0 / num_starts\n",
    "            earliest_start = min(start_nodes)\n",
    "            for node in start_nodes:\n",
    "                prob += node_act_vars[str(node)] == initial_energy, 'InitialNode' + str(node)\n",
    "            for node in range(0, earliest_start):\n",
    "                prob += node_act_vars[str(node)] == 0, 'BeforeStart' + str(node)\n",
    "    if has_end:\n",
    "        num_ends = len(end_nodes)\n",
    "        if verbose:\n",
    "            print(\"End node(s):\")\n",
    "            print(end_nodes)\n",
    "        if num_ends == 0:  # This is the default when no list is given and it has a start.\n",
    "            prob += node_act_vars[str(n - 1)] == 1, 'FinalNode'\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Added end node(s)\")\n",
    "                print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "            final_energy = 1.0 / num_ends\n",
    "            latest_end = min(end_nodes)\n",
    "            for node in end_nodes:\n",
    "                prob += node_act_vars[str(node)] == final_energy, 'FinalNode' + str(node)\n",
    "            for node in range(latest_end + 1, n):\n",
    "                prob += node_act_vars[str(node)] == 0, 'AfterEnd' + str(node)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Chain constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    prob += lpSum([node_act_vars[i] for i in var_i]) == K, 'KNodes'\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Expected length constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if has_start:\n",
    "        if verbose:\n",
    "            print(\"Equality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for j in range(1, n):\n",
    "            if j not in start_nodes:\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for i in window_j_i[j]]) == node_act_vars[str(j)], 'InEdgeReq' + str(j)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Generating specific starting node constraints.\")\n",
    "                    print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for i in window_j_i[j]]) == 0, 'InEdgeReq' + str(j)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Inequality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for j in range(1, n):\n",
    "            prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                          for i in window_j_i[j]]) <= node_act_vars[str(j)], 'InEdgeReq' + str(j)\n",
    "    if verbose:\n",
    "        print(\"In-degree constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if has_end:\n",
    "        if verbose:\n",
    "            print(\"Equality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for i in range(0, n - 1):\n",
    "            if i not in end_nodes:\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for j in window_i_j[i]]) == node_act_vars[str(i)], 'OutEdgeReq' + str(i)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Generating specific starting node constraints.\")\n",
    "                    print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "                prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                              for j in window_i_j[i]]) == 0, 'OutEdgeReq' + str(i)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Inequality constraints.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for i in range(0, n - 1):\n",
    "            prob += lpSum([node_next_vars[str(i) + \"_\" + str(j)]\n",
    "                          for j in window_i_j[i]]) <= node_act_vars[str(i)], 'OutEdgeReq' + str(i)\n",
    "    if verbose:\n",
    "        print(\"Out-degree constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # Objective\n",
    "    for i in range(0, n):\n",
    "        for j in window_i_j[i]:\n",
    "            coherence_weights = [0.5, 0.5]\n",
    "            # Five or more entities in common means double the connection strength.\n",
    "            entity_multiplier = min(1 + ent_table[i, j], 2)\n",
    "            # Geometric mean the relevances, multiply based on how far it is from 0.5.\n",
    "            relevance_multiplier = (relevance_table[i] * relevance_table[j]) ** 0.5\n",
    "            coherence = (sim_table[i, j] ** coherence_weights[0]) * \\\n",
    "                (clust_sim_table[i, j] ** coherence_weights[1])\n",
    "            weighted_coherence = min(coherence * entity_multiplier * relevance_multiplier, 1.0)\n",
    "            prob += minedge <= 1 - node_next_vars[str(i) + \"_\" + str(j)] + \\\n",
    "                weighted_coherence, \"Objective\" + str(i) + \"_\" + str(j)\n",
    "    if verbose:\n",
    "        print(\"Objective constraints created.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if previous_varsdict:\n",
    "        current_names = [v.name for v in prob.variables() if \"node_act\" in v.name]\n",
    "        if verbose:\n",
    "            print(\"Generated list of names.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        for k, v in previous_varsdict.items():\n",
    "            if \"node_act\" in k and k in current_names:\n",
    "                node_act_vars[k.replace(\"node_act_\", \"\")].setInitialValue(v)\n",
    "\n",
    "    if verbose:\n",
    "        if previous_varsdict:\n",
    "            print(\"Used previous solution as starting point.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "        else:\n",
    "            print(\"No previous solution available.\")\n",
    "            print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    # The problem data is written to an .lp file\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05d7ca",
   "metadata": {},
   "source": [
    "## Building the Graph Data Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920aa6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_df_multiple_starts(query, varsdict, prune=None, threshold=0.01, cluster_dict={}, start_nodes=[]):\n",
    "    n = len(query)\n",
    "    # This has some leftover stuff that is not really useful now.\n",
    "    if 'bias' in query.columns:\n",
    "        graph_df = pd.DataFrame(columns=['id', 'adj_list', 'adj_weights',\n",
    "                                'date', 'publication', 'title', 'text', 'url', 'bias', 'coherence'])\n",
    "    else:\n",
    "        graph_df = pd.DataFrame(columns=['id', 'adj_list', 'adj_weights',\n",
    "                                'date', 'publication', 'title', 'text', 'url', 'coherence'])\n",
    "\n",
    "    already_in = []\n",
    "    for i in range(0, n):\n",
    "        prob = []\n",
    "        coherence = varsdict[\"node_act_\" + str(i)]\n",
    "        if coherence <= threshold:\n",
    "            continue\n",
    "        coherence_list = []\n",
    "        index_list = []\n",
    "        for j in window_i_j[i]:\n",
    "            name = \"node_next_\" + str(i) + \"_\" + str(j)\n",
    "            prob.append(varsdict[name])\n",
    "            coherence_list.append(varsdict[\"node_act_\" + str(j)])\n",
    "        idx_list = [window_i_j[i][idx] for idx, e in enumerate(prob) if round(\n",
    "            e, 8) != 0 and e > threshold and coherence_list[idx] > threshold]  # idx + i + 1\n",
    "        nz_prob = [e for idx, e in enumerate(prob) if round(\n",
    "            e, 8) != 0 and e > threshold and coherence_list[idx] > threshold]\n",
    "        if prune:\n",
    "            if len(idx_list) > prune:\n",
    "                top_prob_idx = sorted(range(len(nz_prob)), key=lambda k: nz_prob[k])[-prune:]\n",
    "                idx_list = [idx_list[j] for j in top_prob_idx]\n",
    "                nz_prob = [nz_prob[idx] for idx in top_prob_idx]\n",
    "        sum_nz = sum(nz_prob)\n",
    "        nz_prob = [nz_prob[j] / sum_nz for j in range(0, len(nz_prob))]\n",
    "        # If we haven't checked this one before we add it to the graph.\n",
    "        url = str(query.iloc[i]['url'])\n",
    "        if i in already_in or sum_nz > 0:\n",
    "            if len(url) > 0:\n",
    "                url = urlparse(url).netloc\n",
    "            if not (graph_df['id'] == i).any():\n",
    "                title = query.iloc[i]['title']\n",
    "                for key, value in cluster_dict.items():\n",
    "                    if str(i) in value:\n",
    "                        title = \"[\" + str(key) + \"] \" + title\n",
    "                outgoing_edges = [idx_temp for idx_temp in idx_list]\n",
    "                # coherence = varsdict[\"node_act_\" + str(i)]\n",
    "                if 'bias' in query.columns:\n",
    "                    graph_df.loc[len(graph_df)] = [i, outgoing_edges, nz_prob, query.iloc[i]['date'], query.iloc[i]['publication'],\n",
    "                                                   title, '', query.iloc[i]['url'], query.iloc[i]['bias'], coherence]\n",
    "                else:\n",
    "                    graph_df.loc[len(graph_df)] = [i, outgoing_edges, nz_prob, query.iloc[i]['date'], query.iloc[i]['publication'],\n",
    "                                                   title, '', query.iloc[i]['url'], coherence]\n",
    "\n",
    "            already_in += [i] + idx_list\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b2a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = None\n",
    "window_i_j = {}\n",
    "window_j_i = {}\n",
    "\n",
    "\n",
    "def solve_LP(\n",
    "    query,\n",
    "    dataset,\n",
    "    membership_vectors,\n",
    "    K=6,\n",
    "    mincover=0.20,\n",
    "    sigma_t=30,\n",
    "    start_nodes=[],\n",
    "    end_nodes=[],\n",
    "    verbose=True,\n",
    "    force_cluster=True,\n",
    "    use_entities=True,\n",
    "    use_temporal=True,\n",
    "    strict_start=False,\n",
    "):\n",
    "\n",
    "    global start_time\n",
    "    start_time = time()\n",
    "\n",
    "    n = len(query.index)\n",
    "    # varsdict_filename = 'varsdict_' + dataset + \"_\" + str(n) + '.pickle'\n",
    "\n",
    "    if sigma_t != 0 and use_temporal:\n",
    "        exp_temp_table = np.exp(-temporal_distance_table / sigma_t)\n",
    "    else:\n",
    "        exp_temp_table = np.ones(temporal_distance_table.shape)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed temporal distance table.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    window_time = None\n",
    "    if sigma_t != 0 and use_temporal:\n",
    "        window_time = sigma_t * 3  # Days\n",
    "\n",
    "    if window_time is None:\n",
    "        for i in range(0, n):\n",
    "            window_i_j[i] = list(range(i + 1, n))\n",
    "        for j in range(0, n):\n",
    "            window_j_i[j] = list(range(0, j))\n",
    "    else:\n",
    "        for j in range(0, n):\n",
    "            window_j_i[j] = []\n",
    "        for i in range(0, n):\n",
    "            window_i_j[i] = []\n",
    "        for i in range(0, n - 1):\n",
    "            window = 0\n",
    "            for j in range(i + 1, n):\n",
    "                if temporal_distance_table[i, j] <= window_time:\n",
    "                    window += 1\n",
    "            window = max(min(5, n - i), window)\n",
    "            window_i_j[i] = list(range(i + 1, min(i + window, n)))\n",
    "            for j in window_i_j[i]:\n",
    "                window_j_i[j].append(i)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed temporal windows.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computed entity similarities.\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "    ent_table = np.zeros((n, n))  # Fill entity information with zeros by default.\n",
    "    actual_ent_table = ent_table\n",
    "    ent_doc_list = None\n",
    "    if use_entities:\n",
    "        ent_table, ent_doc_list = get_entity_table(query, dataset)\n",
    "        actual_ent_table = ent_table\n",
    "\n",
    "    # Deprecated relevance table computation\n",
    "    relevance_table = [1.0] * membership_vectors.shape[0]  # Create a vector full of 1s\n",
    "\n",
    "    has_start = False\n",
    "    if start_nodes is not None:\n",
    "        has_start = (len(start_nodes) > 0)\n",
    "    if end_nodes is not None:\n",
    "        has_end = (len(end_nodes) > 0)\n",
    "    if verbose:\n",
    "        print(\"Creating LP...\")\n",
    "\n",
    "    # Read previous solution and feed to LP. If none there is no previous solution.\n",
    "    previous_varsdict = None\n",
    "    # if os.path.isfile(varsdict_filename):\n",
    "    #     with open(varsdict_filename, 'rb') as handle:\n",
    "    #         previous_varsdict = pickle.load(handle)\n",
    "\n",
    "    prob = create_LP(\n",
    "        query,\n",
    "        sim_table,\n",
    "        membership_vectors,\n",
    "        clust_sim_table,\n",
    "        exp_temp_table,\n",
    "        actual_ent_table,\n",
    "        numclust,\n",
    "        relevance_table,\n",
    "        K=K,\n",
    "        mincover=mincover,\n",
    "        sigma_t=sigma_t,\n",
    "        has_start=has_start,\n",
    "        has_end=has_end,\n",
    "        start_nodes=start_nodes,\n",
    "        end_nodes=end_nodes,\n",
    "        verbose=verbose,\n",
    "        force_cluster=force_cluster,\n",
    "        previous_varsdict=previous_varsdict\n",
    "    )\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(\"Saving model...\")\n",
    "    #     print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # prob.writeLP(\"left_story.lp\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Solving model...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    # (GLPK_CMD(path = 'C:\\\\glpk-4.65\\\\w64\\\\glpsol.exe', options = [\"--tmlim\", \"180\"]))\n",
    "\n",
    "    prob.solve(PULP_CBC_CMD(mip=False, warmStart=True, msg=verbose))\n",
    "\n",
    "    varsdict = extract_varsdict(prob)\n",
    "\n",
    "    # Overwrite last solution.\n",
    "    # with open(varsdict_filename, 'wb') as handle:\n",
    "    #     pickle.dump(varsdict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    graph_df = build_graph_df_multiple_starts(query, varsdict, prune=ceil(\n",
    "        sqrt(K)), threshold=0.1/K, cluster_dict={})\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Graph data frame construction...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    if strict_start and has_start:\n",
    "        graph_df = graph_clean_up(graph_df, start_nodes)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Graph clean up...\")\n",
    "        print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "    return [graph_df, (numclust, LpStatus[prob.status]), sim_table, clust_sim_table, ent_table, ent_doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378d128",
   "metadata": {},
   "source": [
    "## MAIN: Calling the Map Construction Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c530c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../../data/AMiner/embed_data-gpt4.pickle' loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "      <th>embed</th>\n",
       "      <th>publication</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4407</td>\n",
       "      <td>53e9b0abb7602d9703b14d28</td>\n",
       "      <td>An Integrated Query and Mining System for Temp...</td>\n",
       "      <td>10.1007/3-540-44466-1_33</td>\n",
       "      <td>[temporal association rules, better decision-m...</td>\n",
       "      <td>In real world the knowledge used for aiding de...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>[-0.03705746680498123, 0.02978450618684292, 0....</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1718</td>\n",
       "      <td>53e9baecb7602d970471ac79</td>\n",
       "      <td>Recognition of local features for camera-based...</td>\n",
       "      <td>10.1109/ICPR.2000.903050</td>\n",
       "      <td>[hand shape, hand location, global features, c...</td>\n",
       "      <td>A sign language recognition system is required...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>[0.03322494775056839, 0.005597303621470928, 0....</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5394</td>\n",
       "      <td>53e99a0ab7602d9702257cbb</td>\n",
       "      <td>Chunking with WPDV Models.</td>\n",
       "      <td>10.3115/1117601.1117639</td>\n",
       "      <td>[different base chunkers, wpdv model, base chu...</td>\n",
       "      <td>In this paper I describe the application of th...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>[-0.003214028896763921, -0.01248752698302269, ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4937</td>\n",
       "      <td>53e9ae3cb7602d9703852344</td>\n",
       "      <td>Fast implementation of multiple oriented filters</td>\n",
       "      <td>10.1109/ICPR.2000.903582</td>\n",
       "      <td>[optimisation, binary restoration, time comple...</td>\n",
       "      <td>One method to estimate image values in the pre...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>[-0.023320039734244347, 0.021940156817436218, ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2798</td>\n",
       "      <td>53e9bc9db7602d970491c40d</td>\n",
       "      <td>Agent negotiation in trusted third party media...</td>\n",
       "      <td>10.1145/336595.337482</td>\n",
       "      <td>[uncertainty equivalent, rational agents, trus...</td>\n",
       "      <td>Traditional game theoretic reasoning for agent...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>[-0.060538649559020996, -0.025153733789920807,...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>2006</td>\n",
       "      <td>6361e3d390e50fcafd8ca03e</td>\n",
       "      <td>Fuzzy Cognitive Maps for Interpretable Image-b...</td>\n",
       "      <td>10.1109/FUZZ-IEEE55066.2022.9882767</td>\n",
       "      <td>[fuzzy cognitive maps, fuzzy sets, interpretab...</td>\n",
       "      <td>Image classification is a fundamental componen...</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[0.002480722963809967, -0.012560468167066574, ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>4983</td>\n",
       "      <td>61bc2f585244ab9dcb0e9e6a</td>\n",
       "      <td>Using deep clustering to improve fMRI dynamic ...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119288</td>\n",
       "      <td>[Dynamic functional connectivity, Sliding wind...</td>\n",
       "      <td>•We compared dimensionality reduction methods ...</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[-0.0012515292037278414, -0.01167546771466732,...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>1500</td>\n",
       "      <td>61dab91e5244ab9dcb5ab654</td>\n",
       "      <td>Efficient deep-reinforcement learning aware re...</td>\n",
       "      <td>10.1007/s10515-021-00318-6</td>\n",
       "      <td>[Deep reinforcement learning,  Offloading,  Sc...</td>\n",
       "      <td>These days, fog computing is an emerging parad...</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[0.015906620770692825, -0.008914765901863575, ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>245</td>\n",
       "      <td>62d16f3c5aee126c0fd7a8f0</td>\n",
       "      <td>Adapting Deep Learning for Content Caching Fra...</td>\n",
       "      <td>10.1109/OJCOMS.2022.3175927</td>\n",
       "      <td>[Traffic offloading, D2D caching, machine lear...</td>\n",
       "      <td>Recently, we have witnessed an expeditious gro...</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[0.03156762197613716, -0.031177597120404243, 0...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1265</td>\n",
       "      <td>628d29e65aee126c0f515acd</td>\n",
       "      <td>A new design of a flying robot, with advanced ...</td>\n",
       "      <td>10.1016/j.jksuci.2020.07.009</td>\n",
       "      <td>[Flying robot, Self-maintenance, Smart grid, C...</td>\n",
       "      <td>In this paper, we present a full design of a f...</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[0.0289794709533453, 0.023744959384202957, 0.0...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                        id  \\\n",
       "0     4407  53e9b0abb7602d9703b14d28   \n",
       "1     1718  53e9baecb7602d970471ac79   \n",
       "2     5394  53e99a0ab7602d9702257cbb   \n",
       "3     4937  53e9ae3cb7602d9703852344   \n",
       "4     2798  53e9bc9db7602d970491c40d   \n",
       "...    ...                       ...   \n",
       "1135  2006  6361e3d390e50fcafd8ca03e   \n",
       "1136  4983  61bc2f585244ab9dcb0e9e6a   \n",
       "1137  1500  61dab91e5244ab9dcb5ab654   \n",
       "1138   245  62d16f3c5aee126c0fd7a8f0   \n",
       "1139  1265  628d29e65aee126c0f515acd   \n",
       "\n",
       "                                                  title  \\\n",
       "0     An Integrated Query and Mining System for Temp...   \n",
       "1     Recognition of local features for camera-based...   \n",
       "2                            Chunking with WPDV Models.   \n",
       "3      Fast implementation of multiple oriented filters   \n",
       "4     Agent negotiation in trusted third party media...   \n",
       "...                                                 ...   \n",
       "1135  Fuzzy Cognitive Maps for Interpretable Image-b...   \n",
       "1136  Using deep clustering to improve fMRI dynamic ...   \n",
       "1137  Efficient deep-reinforcement learning aware re...   \n",
       "1138  Adapting Deep Learning for Content Caching Fra...   \n",
       "1139  A new design of a flying robot, with advanced ...   \n",
       "\n",
       "                                      doi  \\\n",
       "0                10.1007/3-540-44466-1_33   \n",
       "1                10.1109/ICPR.2000.903050   \n",
       "2                 10.3115/1117601.1117639   \n",
       "3                10.1109/ICPR.2000.903582   \n",
       "4                   10.1145/336595.337482   \n",
       "...                                   ...   \n",
       "1135  10.1109/FUZZ-IEEE55066.2022.9882767   \n",
       "1136     10.1016/j.neuroimage.2022.119288   \n",
       "1137           10.1007/s10515-021-00318-6   \n",
       "1138          10.1109/OJCOMS.2022.3175927   \n",
       "1139         10.1016/j.jksuci.2020.07.009   \n",
       "\n",
       "                                               keywords  \\\n",
       "0     [temporal association rules, better decision-m...   \n",
       "1     [hand shape, hand location, global features, c...   \n",
       "2     [different base chunkers, wpdv model, base chu...   \n",
       "3     [optimisation, binary restoration, time comple...   \n",
       "4     [uncertainty equivalent, rational agents, trus...   \n",
       "...                                                 ...   \n",
       "1135  [fuzzy cognitive maps, fuzzy sets, interpretab...   \n",
       "1136  [Dynamic functional connectivity, Sliding wind...   \n",
       "1137  [Deep reinforcement learning,  Offloading,  Sc...   \n",
       "1138  [Traffic offloading, D2D caching, machine lear...   \n",
       "1139  [Flying robot, Self-maintenance, Smart grid, C...   \n",
       "\n",
       "                                               abstract       date  \\\n",
       "0     In real world the knowledge used for aiding de... 2000-01-01   \n",
       "1     A sign language recognition system is required... 2000-01-01   \n",
       "2     In this paper I describe the application of th... 2000-01-01   \n",
       "3     One method to estimate image values in the pre... 2000-01-01   \n",
       "4     Traditional game theoretic reasoning for agent... 2000-01-01   \n",
       "...                                                 ...        ...   \n",
       "1135  Image classification is a fundamental componen... 2022-01-01   \n",
       "1136  •We compared dimensionality reduction methods ... 2022-01-01   \n",
       "1137  These days, fog computing is an emerging parad... 2022-01-01   \n",
       "1138  Recently, we have witnessed an expeditious gro... 2022-01-01   \n",
       "1139  In this paper, we present a full design of a f... 2022-01-01   \n",
       "\n",
       "                                                  embed publication url  \n",
       "0     [-0.03705746680498123, 0.02978450618684292, 0....                  \n",
       "1     [0.03322494775056839, 0.005597303621470928, 0....                  \n",
       "2     [-0.003214028896763921, -0.01248752698302269, ...                  \n",
       "3     [-0.023320039734244347, 0.021940156817436218, ...                  \n",
       "4     [-0.060538649559020996, -0.025153733789920807,...                  \n",
       "...                                                 ...         ...  ..  \n",
       "1135  [0.002480722963809967, -0.012560468167066574, ...                  \n",
       "1136  [-0.0012515292037278414, -0.01167546771466732,...                  \n",
       "1137  [0.015906620770692825, -0.008914765901863575, ...                  \n",
       "1138  [0.03156762197613716, -0.031177597120404243, 0...                  \n",
       "1139  [0.0289794709533453, 0.023744959384202957, 0.0...                  \n",
       "\n",
       "[1140 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aminer = pd.read_feather(f\"../../data/AMiner/aminer-subset.feather\")\n",
    "data_aminer[\"date\"] = pd.to_datetime(data_aminer[\"date\"])\n",
    "data_aminer.reset_index(inplace=True, drop=True)  # The original indices are incorrect, so we reset here.\n",
    "\n",
    "# Load embeddings for Aminer\n",
    "aminer_embeds, _, _ = extract_embeddings(\n",
    "    text=(data_aminer[\"title\"] + \";\" + data_aminer[\"abstract\"]).tolist(),\n",
    "    foldername=f\"../../data/AMiner\",\n",
    "    model_name=\"gpt4\"\n",
    ")\n",
    "\n",
    "data_aminer[\"embed\"] = aminer_embeds.tolist()\n",
    "data_aminer[\"publication\"] = \"\"\n",
    "data_aminer[\"url\"] = \"\"\n",
    "data_aminer.sort_values(by=\"date\").reset_index(names=\"idx\")\n",
    "\n",
    "\n",
    "data_aminer = data_aminer.sample(1140, replace=False, random_state=420).sort_values(by=\"date\")\n",
    "data_aminer.reset_index(names=\"idx\", inplace=True)\n",
    "\n",
    "\n",
    "# select sources and targets\n",
    "aminer_src = np.random.choice(data_aminer.index[:(len(data_aminer) // 2) - 50], 50)\n",
    "aminer_tgt = np.random.choice(data_aminer.index[(len(data_aminer) // 2) + 50:], 50)\n",
    "\n",
    "data_aminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5193cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 Topics\n"
     ]
    }
   ],
   "source": [
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=32,\n",
    "    n_components=48,\n",
    "    min_dist=0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    n_jobs=1,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "low_dim_mapper = umap_model.fit(np.array(data_aminer[\"embed\"].tolist()))\n",
    "low_dim_embeds = low_dim_mapper.embedding_\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ").fit(low_dim_embeds)\n",
    "\n",
    "cluster_label_probs = hdbscan.prediction.all_points_membership_vectors(\n",
    "    hdbscan_model\n",
    ")\n",
    "\n",
    "cluster_labels = cluster_label_probs.argmax(1)\n",
    "\n",
    "data_aminer[\"topic\"] = cluster_labels\n",
    "print(f\"Found {len(np.unique(cluster_labels))} Topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29150ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Length (usually values from 6 to 12 produce decent maps, but it depends on data set size and probably the underlying distribution of similarities).\n",
    "k_input = 12\n",
    "\n",
    "# % of average coverage we require. For small data sets 50-80 is good. For bigger data sets with many clusters you will likely only get 20%.\n",
    "# This was tested with values up to 500. After that I'm not sure how well the model will perform.\n",
    "mincover_input = 0\n",
    "\n",
    "# Temporal distance penalty in DAYS. I left it on 30 as default for the Cuban data set.\n",
    "# Lower values allow more temporally distant connections. Consider temporal density of the data when adjusting.\n",
    "# Can set it to 0 and it will be discarded from the computation.\n",
    "sigma_t = 0\n",
    "use_temporal = False  # Use this to enable or disable the temporal penalty, by default it is on.\n",
    "\n",
    "# Leave this as false, there was supposed to be a reward factor for events with common entities, but it adds too much computational time so not worth it.\n",
    "use_entities = False\n",
    "\n",
    "# If you enable strict start you will discard any storyline that does not start from the user-defined start node.\n",
    "# It is recommended to disable this to allow for extra storylines that emerge from the LP solution.\n",
    "strict_start = False\n",
    "\n",
    "# Compute angular similarity\n",
    "similarities = np.clip(cosine_similarity(np.array(data_aminer[\"embed\"].tolist())), -1, 1)\n",
    "sim_table = (1 - np.arccos(similarities) / pi)\n",
    "mask = np.ones(sim_table.shape, dtype=bool)\n",
    "np.fill_diagonal(mask, 0)\n",
    "max_value = sim_table[mask].max()\n",
    "min_value = sim_table[mask].min()\n",
    "sim_table = (sim_table - min_value) / (max_value - min_value)\n",
    "sim_table = np.clip(sim_table, 0, 1)\n",
    "\n",
    "# Compute topic similarity\n",
    "numclust = 1\n",
    "clust_sim = np.zeros((cluster_label_probs.shape[0], cluster_label_probs.shape[0]))\n",
    "\n",
    "if len(cluster_label_probs.shape) > 1:\n",
    "    numclust = cluster_label_probs.shape[1]\n",
    "    cluster_label_probs[cluster_label_probs < 1/numclust] = 0\n",
    "    cluster_label_probs[np.all(cluster_label_probs == 0,\n",
    "                                        axis=1)] = np.ones(numclust) / numclust\n",
    "    row_sums = cluster_label_probs.sum(axis=1)\n",
    "    cluster_label_probs = cluster_label_probs / row_sums[:, np.newaxis]\n",
    "\n",
    "    clust_sim = distance.cdist(\n",
    "        cluster_label_probs,\n",
    "        cluster_label_probs,\n",
    "        lambda u, v: distance.jensenshannon(u, v, base=2.0)\n",
    "    )\n",
    "else:\n",
    "    cluster_label_probs = np.ones((cluster_label_probs.shape[0], 1))\n",
    "\n",
    "clust_sim_table = 1 - clust_sim\n",
    "\n",
    "# Compute temporal distance\n",
    "temporal_distance_table = compute_temp_distance_table(data_aminer, \"./narrative_maps/temp/aminer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78300ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [3:10:16<00:00, 228.33s/it]  \n"
     ]
    }
   ],
   "source": [
    "results_data = {\n",
    "    \"algorithm\": [],\n",
    "    \"src\": [],\n",
    "    \"tgt\": [],\n",
    "    \"exec_time\": [],\n",
    "    \"effective_exec_time\": [],\n",
    "    \"main_storyline\": [],\n",
    "    \"storylines\": []\n",
    "}\n",
    "\n",
    "for src, tgt in tqdm(zip(aminer_src, aminer_tgt), total=len(aminer_src)):\n",
    "    for i in range(4):\n",
    "        start_time = time()\n",
    "        graph_df_new, status, _, _, _, _ = solve_LP(\n",
    "            data_aminer,\n",
    "            dataset=\"news_articles\",\n",
    "            membership_vectors=cluster_label_probs,\n",
    "            K=k_input,\n",
    "            mincover=mincover_input/100,\n",
    "            sigma_t=sigma_t,\n",
    "            start_nodes=[src],\n",
    "            end_nodes=[tgt],\n",
    "            verbose=False,\n",
    "            use_entities=use_entities,\n",
    "            use_temporal=use_temporal,\n",
    "            strict_start=strict_start,\n",
    "        )\n",
    "        end_time = time() - start_time\n",
    "\n",
    "        # Post Processing\n",
    "        if 'Optimal' in status[1]:\n",
    "            G = build_graph(graph_df_new)\n",
    "            storylines = graph_stories(G, start_nodes=[src], end_nodes=[tgt])\n",
    "\n",
    "            results_data[\"algorithm\"].append(\"narrative_maps\")\n",
    "            results_data[\"src\"].append(src)\n",
    "            results_data[\"tgt\"].append(tgt)\n",
    "            results_data[\"exec_time\"].append(end_time)\n",
    "            results_data[\"effective_exec_time\"].append(end_time / len(storylines))\n",
    "            results_data[\"main_storyline\"].append(storylines[0])\n",
    "            results_data[\"storylines\"].append(storylines)\n",
    "        else:\n",
    "            print(f\"** Warning: Experiment '({src}, {tgt})' not optimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644f252f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>exec_time</th>\n",
       "      <th>effective_exec_time</th>\n",
       "      <th>main_storyline</th>\n",
       "      <th>storylines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>226</td>\n",
       "      <td>729</td>\n",
       "      <td>55.071710</td>\n",
       "      <td>5.507171</td>\n",
       "      <td>[226, 406, 541, 579, 665, 729]</td>\n",
       "      <td>[[226, 406, 541, 579, 665, 729], [252, 324, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>218</td>\n",
       "      <td>784</td>\n",
       "      <td>52.782767</td>\n",
       "      <td>4.798433</td>\n",
       "      <td>[218, 235, 556, 592, 605, 660, 676, 732, 784]</td>\n",
       "      <td>[[218, 235, 556, 592, 605, 660, 676, 732, 784]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>175</td>\n",
       "      <td>653</td>\n",
       "      <td>58.577506</td>\n",
       "      <td>4.184108</td>\n",
       "      <td>[175, 381, 519, 556, 580, 626, 644, 653]</td>\n",
       "      <td>[[175, 381, 519, 556, 580, 626, 644, 653], [63...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>78</td>\n",
       "      <td>648</td>\n",
       "      <td>57.978055</td>\n",
       "      <td>3.623628</td>\n",
       "      <td>[78, 299, 502, 648]</td>\n",
       "      <td>[[78, 299, 502, 648], [244, 272], [120, 196, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>329</td>\n",
       "      <td>939</td>\n",
       "      <td>53.571004</td>\n",
       "      <td>5.357100</td>\n",
       "      <td>[329, 468, 503, 519, 520, 558, 693, 716, 748, ...</td>\n",
       "      <td>[[329, 468, 503, 519, 520, 558, 693, 716, 748,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>328</td>\n",
       "      <td>1025</td>\n",
       "      <td>52.419744</td>\n",
       "      <td>4.032288</td>\n",
       "      <td>[328, 427, 504, 677, 827, 971, 999, 1025]</td>\n",
       "      <td>[[328, 427, 504, 677, 827, 971, 999, 1025], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>226</td>\n",
       "      <td>729</td>\n",
       "      <td>55.351980</td>\n",
       "      <td>5.535198</td>\n",
       "      <td>[226, 406, 541, 579, 665, 729]</td>\n",
       "      <td>[[226, 406, 541, 579, 665, 729], [252, 324, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>81</td>\n",
       "      <td>1119</td>\n",
       "      <td>62.483050</td>\n",
       "      <td>4.806388</td>\n",
       "      <td>[81, 567, 666, 684, 779, 785, 917, 929, 1045, ...</td>\n",
       "      <td>[[81, 567, 666, 684, 779, 785, 917, 929, 1045,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>328</td>\n",
       "      <td>1025</td>\n",
       "      <td>52.022058</td>\n",
       "      <td>4.001697</td>\n",
       "      <td>[328, 427, 504, 677, 827, 971, 999, 1025]</td>\n",
       "      <td>[[328, 427, 504, 677, 827, 971, 999, 1025], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>narrative_maps</td>\n",
       "      <td>460</td>\n",
       "      <td>1091</td>\n",
       "      <td>58.294845</td>\n",
       "      <td>3.886323</td>\n",
       "      <td>[460, 522, 577, 580, 795, 922, 929, 1091]</td>\n",
       "      <td>[[460, 522, 577, 580, 795, 922, 929, 1091], [9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          algorithm  src   tgt  exec_time  effective_exec_time  \\\n",
       "81   narrative_maps  226   729  55.071710             5.507171   \n",
       "124  narrative_maps  218   784  52.782767             4.798433   \n",
       "11   narrative_maps  175   653  58.577506             4.184108   \n",
       "68   narrative_maps   78   648  57.978055             3.623628   \n",
       "85   narrative_maps  329   939  53.571004             5.357100   \n",
       "25   narrative_maps  328  1025  52.419744             4.032288   \n",
       "83   narrative_maps  226   729  55.351980             5.535198   \n",
       "148  narrative_maps   81  1119  62.483050             4.806388   \n",
       "26   narrative_maps  328  1025  52.022058             4.001697   \n",
       "30   narrative_maps  460  1091  58.294845             3.886323   \n",
       "\n",
       "                                        main_storyline  \\\n",
       "81                      [226, 406, 541, 579, 665, 729]   \n",
       "124      [218, 235, 556, 592, 605, 660, 676, 732, 784]   \n",
       "11            [175, 381, 519, 556, 580, 626, 644, 653]   \n",
       "68                                 [78, 299, 502, 648]   \n",
       "85   [329, 468, 503, 519, 520, 558, 693, 716, 748, ...   \n",
       "25           [328, 427, 504, 677, 827, 971, 999, 1025]   \n",
       "83                      [226, 406, 541, 579, 665, 729]   \n",
       "148  [81, 567, 666, 684, 779, 785, 917, 929, 1045, ...   \n",
       "26           [328, 427, 504, 677, 827, 971, 999, 1025]   \n",
       "30           [460, 522, 577, 580, 795, 922, 929, 1091]   \n",
       "\n",
       "                                            storylines  \n",
       "81   [[226, 406, 541, 579, 665, 729], [252, 324, 34...  \n",
       "124  [[218, 235, 556, 592, 605, 660, 676, 732, 784]...  \n",
       "11   [[175, 381, 519, 556, 580, 626, 644, 653], [63...  \n",
       "68   [[78, 299, 502, 648], [244, 272], [120, 196, 2...  \n",
       "85   [[329, 468, 503, 519, 520, 558, 693, 716, 748,...  \n",
       "25   [[328, 427, 504, 677, 827, 971, 999, 1025], [5...  \n",
       "83   [[226, 406, 541, 579, 665, 729], [252, 324, 34...  \n",
       "148  [[81, 567, 666, 684, 779, 785, 917, 929, 1045,...  \n",
       "26   [[328, 427, 504, 677, 827, 971, 999, 1025], [5...  \n",
       "30   [[460, 522, 577, 580, 795, 922, 929, 1091], [9...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_data_df = pd.DataFrame(results_data)\n",
    "results_data_df.sample(10)  # Show 10 example results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8273d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data_df.to_pickle(\"./narrative_maps/results/aminer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9add7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrative-trails",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
